
<!doctype html>
<html lang="zh" class="no-js">
<head>
    
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    
    <meta name="description" content="本章节将介绍如何使用大模型进行训练，以及如何使用大模型进行推理、以及如何进行模型压缩、加速训练、提升性能、大模型训练框架等。">
    
    
    
    <link rel="canonical" href="https://docs.kweaver.ai/advanced/big_models/">
    
    
    <link rel="prev" href="../agents/">
    
    
    <link rel="next" href="../knowledge_graphs/">
    
    
    <link rel="icon" href="../../assets/favicon.png">
    <meta name="generator" content="mkdocs-1.6.0, mkdocs-material-9.4.7">
    
    
    
    <title>大模型知识 - KWeaver官方技术手册</title>
    
    
    
    <link rel="stylesheet" href="../../assets/stylesheets/main.4b4a2bd9.min.css">
    
    
    <link rel="stylesheet" href="../../assets/stylesheets/palette.356b1318.min.css">
    
    


    
    
    
    
    
    
    
    
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
    <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
    
    
    
    <link rel="stylesheet" href="../../assets/extra.css">
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
    

    
    
    
</head>









<body dir="ltr" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo">



<script>var media,input,key,value,palette=__md_get("__palette");if(palette&&palette.color){"(prefers-color-scheme)"===palette.color.media&&(media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']"),palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent"));for([key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>

<input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
<input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
<label class="md-overlay" for="__drawer"></label>
<div data-md-component="skip">
    
    
    <a href="#_1" class="md-skip">
        跳转至
    </a>
    
</div>
<div data-md-component="announce">
    
</div>

<div data-md-color-scheme="default" data-md-component="outdated" hidden>
    
</div>




  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="页眉">
    <a href="../.." title="KWeaver官方技术手册" class="md-header__button md-logo" aria-label="KWeaver官方技术手册" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 89 89">
  <path d="M3.136,17.387l0,42.932l42.932,21.467l-42.932,-64.399Z" />
  <path d="M21.91,8l42.933,64.398l-18.775,9.388l-42.932,-64.399l18.774,-9.387Z" style="fill-opacity: 0.5" />
  <path d="M67.535,17.387l-27.262,18.156l21.878,32.818l5.384,2.691l0,-53.665Z" />
  <path d="M67.535,17.387l0,53.666l18.774,-9.388l0,-53.665l-18.774,9.387Z" style="fill-opacity: 0.25" />
</svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            KWeaver官方技术手册
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              大模型知识
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69Z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="black" data-md-color-accent="indigo"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12c0-2.42-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69Z"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var media,input,key,value,palette=__md_get("__palette");if(palette&&palette.color){"(prefers-color-scheme)"===palette.color.media&&(media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']"),palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent"));for([key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
      <div class="md-header__option">
  <div class="md-select">
    
    <button class="md-header__button md-icon" aria-label="选择当前语言">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m12.87 15.07-2.54-2.51.03-.03A17.52 17.52 0 0 0 14.07 6H17V4h-7V2H8v2H1v2h11.17C11.5 7.92 10.44 9.75 9 11.35 8.07 10.32 7.3 9.19 6.69 8h-2c.73 1.63 1.73 3.17 2.98 4.56l-5.09 5.02L4 19l5-5 3.11 3.11.76-2.04M18.5 10h-2L12 22h2l1.12-3h4.75L21 22h2l-4.5-12m-2.62 7 1.62-4.33L19.12 17h-3.24Z"/></svg>
    </button>
    <div class="md-select__inner">
      <ul class="md-select__list">
        
          <li class="md-select__item">
            <a href="../../en/advanced/big_models/" hreflang="en" class="md-select__link">
              English
            </a>
          </li>
        
          <li class="md-select__item">
            <a href="./" hreflang="zh" class="md-select__link">
              中文
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</div>
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="搜索" placeholder="搜索" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="查找">
        
          <a href="javascript:void(0)" class="md-search__icon md-icon" title="分享" aria-label="分享" data-clipboard data-clipboard-text="" data-md-component="search-share" tabindex="-1">
            
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7 0-.24-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91 1.61 0 2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08Z"/></svg>
          </a>
        
        <button type="reset" class="md-search__icon md-icon" title="清空当前内容" aria-label="清空当前内容" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            正在初始化搜索引擎
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        <a href="https://github.com/AISHU-Technology/kweaver" title="前往仓库" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.5.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
  </div>
  <div class="md-source__repository">
    KWeaver
  </div>
</a>
      </div>
    
  </nav>
  
</header>

<div class="md-container" data-md-component="container">
    
    
    
    
    
    
    <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
            
            
            
            <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                    <div class="md-sidebar__inner">
                        



<nav class="md-nav md-nav--primary" aria-label="导航栏" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="KWeaver官方技术手册" class="md-nav__button md-logo" aria-label="KWeaver官方技术手册" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 89 89">
  <path d="M3.136,17.387l0,42.932l42.932,21.467l-42.932,-64.399Z" />
  <path d="M21.91,8l42.933,64.398l-18.775,9.388l-42.932,-64.399l18.774,-9.387Z" style="fill-opacity: 0.5" />
  <path d="M67.535,17.387l-27.262,18.156l21.878,32.818l5.384,2.691l0,-53.665Z" />
  <path d="M67.535,17.387l0,53.666l18.774,-9.388l0,-53.665l-18.774,9.387Z" style="fill-opacity: 0.25" />
</svg>

    </a>
    KWeaver官方技术手册
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/AISHU-Technology/kweaver" title="前往仓库" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.5.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
  </div>
  <div class="md-source__repository">
    KWeaver
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    
    
      
        
      
        
      
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_1" >
        
          
          <label class="md-nav__link" for="__nav_1" id="__nav_1_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    简介
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_1">
            <span class="md-nav__icon md-icon"></span>
            简介
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../introduction/about/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    KWeaver介绍
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../introduction/framework/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    架构介绍
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
      
        
      
        
      
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" >
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    快速上手&部署文档
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            快速上手&部署文档
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../getting_started/quick_start/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    快速上手
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../getting_started/deploy_introduction/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    开发部署简介
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_3" >
        
          
          <label class="md-nav__link" for="__nav_2_3" id="__nav_2_3_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    部署手册
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_3">
            <span class="md-nav__icon md-icon"></span>
            部署手册
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../deploy/front_deploy/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    前端开发环境
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../deploy/backend_deploy/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    后端开发环境
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../deploy/docker_compose/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Docker-Compose部署
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../deploy/docker_gpu/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    GPU部署
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../deploy/docker_k8s/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    K8S部署
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
      
        
      
        
      
        
      
        
      
    
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" checked>
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    知识进阶
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            知识进阶
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../AINative/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    AI原生知识
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../agents/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Agent知识
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  <span class="md-ellipsis">
    大模型知识
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    大模型知识
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="目录">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      目录
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#_2" class="md-nav__link">
    <span class="md-ellipsis">
      介绍
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_3" class="md-nav__link">
    <span class="md-ellipsis">
      准备工作
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_4" class="md-nav__link">
    <span class="md-ellipsis">
      数据集
    </span>
  </a>
  
    <nav class="md-nav" aria-label="数据集">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_5" class="md-nav__link">
    <span class="md-ellipsis">
      数据集准备
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_6" class="md-nav__link">
    <span class="md-ellipsis">
      数据集增广
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_7" class="md-nav__link">
    <span class="md-ellipsis">
      数据集采样
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_8" class="md-nav__link">
    <span class="md-ellipsis">
      大模型训练
    </span>
  </a>
  
    <nav class="md-nav" aria-label="大模型训练">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_9" class="md-nav__link">
    <span class="md-ellipsis">
      训练模型
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_10" class="md-nav__link">
    <span class="md-ellipsis">
      常用参数
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_11" class="md-nav__link">
    <span class="md-ellipsis">
      加速训练
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_12" class="md-nav__link">
    <span class="md-ellipsis">
      分布式训练方法
    </span>
  </a>
  
    <nav class="md-nav" aria-label="分布式训练方法">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_13" class="md-nav__link">
    <span class="md-ellipsis">
      数据并行
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_14" class="md-nav__link">
    <span class="md-ellipsis">
      张量并行
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_15" class="md-nav__link">
    <span class="md-ellipsis">
      流水线并行
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3d" class="md-nav__link">
    <span class="md-ellipsis">
      3D 并行
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_16" class="md-nav__link">
    <span class="md-ellipsis">
      其他分布式训练方法
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_17" class="md-nav__link">
    <span class="md-ellipsis">
      训练框架
    </span>
  </a>
  
    <nav class="md-nav" aria-label="训练框架">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#tensorflow" class="md-nav__link">
    <span class="md-ellipsis">
      TensorFlow
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pytorch" class="md-nav__link">
    <span class="md-ellipsis">
      PyTorch
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#deepspeed" class="md-nav__link">
    <span class="md-ellipsis">
      DeepSpeed
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#dlrover" class="md-nav__link">
    <span class="md-ellipsis">
      DLRover
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama-factory" class="md-nav__link">
    <span class="md-ellipsis">
      LLaMa-Factory
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_18" class="md-nav__link">
    <span class="md-ellipsis">
      训练监控
    </span>
  </a>
  
    <nav class="md-nav" aria-label="训练监控">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#swanlab" class="md-nav__link">
    <span class="md-ellipsis">
      SwanLab
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#wandb" class="md-nav__link">
    <span class="md-ellipsis">
      Wandb
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#tensorboard" class="md-nav__link">
    <span class="md-ellipsis">
      TensorBoard
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_19" class="md-nav__link">
    <span class="md-ellipsis">
      评估模型
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_20" class="md-nav__link">
    <span class="md-ellipsis">
      模型量化
    </span>
  </a>
  
    <nav class="md-nav" aria-label="模型量化">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_21" class="md-nav__link">
    <span class="md-ellipsis">
      量化训练
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_22" class="md-nav__link">
    <span class="md-ellipsis">
      量化推理
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_23" class="md-nav__link">
    <span class="md-ellipsis">
      模型压缩
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_24" class="md-nav__link">
    <span class="md-ellipsis">
      加速推理
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_25" class="md-nav__link">
    <span class="md-ellipsis">
      参考资料
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_26" class="md-nav__link">
    <span class="md-ellipsis">
      后续更新
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../knowledge_graphs/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    知识图谱
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
      
        
      
        
      
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" >
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    API
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            API
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../api/models-factory/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    模型工厂
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../api/knowledge_graphs/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    知识网络
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../api/system_management/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    系统管理
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5" >
        
          
          <label class="md-nav__link" for="__nav_5" id="__nav_5_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    FAQ
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5">
            <span class="md-nav__icon md-icon"></span>
            FAQ
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../faq/front_errors/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    前端问题集锦
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../faq/backend_errors/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    后端问题集锦
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../faq/deploy_errors/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    部署问题集锦
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../faq/other_errors/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    其他问题集锦
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
      
        
      
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_5" >
        
          
          <label class="md-nav__link" for="__nav_5_5" id="__nav_5_5_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    常见状态码
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_5_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_5">
            <span class="md-nav__icon md-icon"></span>
            常见状态码
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../error_code/http_status_code/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    http状态码
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../error_code/kw_status_code/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    KWeaver状态码
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
      
        
      
        
      
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6" >
        
          
          <label class="md-nav__link" for="__nav_6" id="__nav_6_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    附录
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_6">
            <span class="md-nav__icon md-icon"></span>
            附录
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../appendix/vocabulary/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    词汇表
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../appendix/version_log/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    版本日志
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../appendix/feedback/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    联系我们
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                    </div>
                </div>
            </div>
            
            
            
            <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                    <div class="md-sidebar__inner">
                        

<nav class="md-nav md-nav--secondary" aria-label="目录">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      目录
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#_2" class="md-nav__link">
    <span class="md-ellipsis">
      介绍
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_3" class="md-nav__link">
    <span class="md-ellipsis">
      准备工作
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_4" class="md-nav__link">
    <span class="md-ellipsis">
      数据集
    </span>
  </a>
  
    <nav class="md-nav" aria-label="数据集">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_5" class="md-nav__link">
    <span class="md-ellipsis">
      数据集准备
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_6" class="md-nav__link">
    <span class="md-ellipsis">
      数据集增广
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_7" class="md-nav__link">
    <span class="md-ellipsis">
      数据集采样
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_8" class="md-nav__link">
    <span class="md-ellipsis">
      大模型训练
    </span>
  </a>
  
    <nav class="md-nav" aria-label="大模型训练">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_9" class="md-nav__link">
    <span class="md-ellipsis">
      训练模型
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_10" class="md-nav__link">
    <span class="md-ellipsis">
      常用参数
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_11" class="md-nav__link">
    <span class="md-ellipsis">
      加速训练
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_12" class="md-nav__link">
    <span class="md-ellipsis">
      分布式训练方法
    </span>
  </a>
  
    <nav class="md-nav" aria-label="分布式训练方法">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_13" class="md-nav__link">
    <span class="md-ellipsis">
      数据并行
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_14" class="md-nav__link">
    <span class="md-ellipsis">
      张量并行
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_15" class="md-nav__link">
    <span class="md-ellipsis">
      流水线并行
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3d" class="md-nav__link">
    <span class="md-ellipsis">
      3D 并行
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_16" class="md-nav__link">
    <span class="md-ellipsis">
      其他分布式训练方法
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_17" class="md-nav__link">
    <span class="md-ellipsis">
      训练框架
    </span>
  </a>
  
    <nav class="md-nav" aria-label="训练框架">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#tensorflow" class="md-nav__link">
    <span class="md-ellipsis">
      TensorFlow
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pytorch" class="md-nav__link">
    <span class="md-ellipsis">
      PyTorch
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#deepspeed" class="md-nav__link">
    <span class="md-ellipsis">
      DeepSpeed
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#dlrover" class="md-nav__link">
    <span class="md-ellipsis">
      DLRover
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama-factory" class="md-nav__link">
    <span class="md-ellipsis">
      LLaMa-Factory
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_18" class="md-nav__link">
    <span class="md-ellipsis">
      训练监控
    </span>
  </a>
  
    <nav class="md-nav" aria-label="训练监控">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#swanlab" class="md-nav__link">
    <span class="md-ellipsis">
      SwanLab
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#wandb" class="md-nav__link">
    <span class="md-ellipsis">
      Wandb
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#tensorboard" class="md-nav__link">
    <span class="md-ellipsis">
      TensorBoard
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_19" class="md-nav__link">
    <span class="md-ellipsis">
      评估模型
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_20" class="md-nav__link">
    <span class="md-ellipsis">
      模型量化
    </span>
  </a>
  
    <nav class="md-nav" aria-label="模型量化">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_21" class="md-nav__link">
    <span class="md-ellipsis">
      量化训练
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_22" class="md-nav__link">
    <span class="md-ellipsis">
      量化推理
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_23" class="md-nav__link">
    <span class="md-ellipsis">
      模型压缩
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_24" class="md-nav__link">
    <span class="md-ellipsis">
      加速推理
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_25" class="md-nav__link">
    <span class="md-ellipsis">
      参考资料
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_26" class="md-nav__link">
    <span class="md-ellipsis">
      后续更新
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                    </div>
                </div>
            </div>
            
            
            
            <div class="md-content" data-md-component="content">
                <article class="md-content__inner md-typeset">
                    <div style="float:right"><span id="busuanzi_container_page_pv"><font size="2" color="grey">本文阅读量<span id="busuanzi_value_page_pv"></span>次</font></span></div>
                    
                    

  
  


<h1 id="_1">大模型知识进阶<a class="headerlink" href="#_1" title="Permanent link">&para;</a></h1>
<h2 id="_2">介绍<a class="headerlink" href="#_2" title="Permanent link">&para;</a></h2>
<p>大模型的训练往往需要大量的计算资源，因此，如何有效地利用这些资源，提升模型的训练速度，是提升模型性能的关键。本章节将介绍如何使用大模型进行训练，以及如何使用大模型进行推理、以及如何进行模型压缩、加速训练、提升性能等。</p>
<h2 id="_3">准备工作<a class="headerlink" href="#_3" title="Permanent link">&para;</a></h2>
<p>本章节将介绍一些常用的方法，来提升模型的训练速度。因此，在开始之前，请确保您已经安装了以下依赖：</p>
<ul>
<li>硬件环境：NVIDIA GPU，CUDA 9.2+，cuDNN 7.6+，NCCL 2.4+</li>
<li>软件环境：Python 3.6+，PyTorch 1.0+，CUDA 9.2+，cuDNN 7.6+，NCCL 2.4+</li>
<li>其他依赖：<ul>
<li>训练数据：需要准备好训练数据，并按照相应的格式组织好数据集。</li>
<li>训练环境：需要准备好训练环境，包括安装 PyTorch、CUDA、cuDNN、NCCL、Apex、TensorRT、DeepSpeed、FairSeq、Ignite、Lightning、PyTorch Hub、TorchServe、TorchText、TorchAudio、TorchVision、TorchData、TorchModelHub 等依赖。</li>
<li>训练脚本：需要准备好训练脚本，包括数据加载、模型定义、损失函数、优化器、训练循环、模型保存等。</li>
<li>训练集群：需要准备好训练集群，包括集群管理系统、GPU 管理系统、分布式训练环境等。</li>
<li>训练策略：需要选择合适的训练策略，包括学习率、权重衰减、模型大小、batch size、epoch 数、优化器、数据增强、模型架构、模型初始化、模型微调、模型蒸馏</li>
<li>训练技巧：需要掌握一些训练技巧，包括模型并行、数据并行、混合精度训练、半精度训练、动态图、XLA、AMP、ONNX、TorchScript、TensorRT、DeepSpeed、FairSeq、Ignite、Lightning、PyTorch Hub、TorchServe、TorchText、TorchAudio、TorchVision、TorchData、TorchModelHub 等。</li>
<li>训练注意事项：需要了解一些训练注意事项，包括模型保存、模型加载、模型微调、模型蒸馏、模型量化、模型剪枝</li>
</ul>
</li>
</ul>
<h2 id="_4">数据集<a class="headerlink" href="#_4" title="Permanent link">&para;</a></h2>
<ul>
<li>数据集（Training Dataset）是指模型训练时，用于训练模型的数据集。</li>
<li>验证集（Validation Set）是指模型训练时，用于验证模型性能的数据集。</li>
<li>测试集（Test Set）是指模型训练后，用于评估模型性能的数据集。</li>
<li>预处理（Preprocessing）是指对数据集进行预处理的方法，包括数据清洗、数据转换、数据归一化等。</li>
<li>增广（Augmentation）是指对数据集进行数据增广的方法，包括数据增强、数据生成等。</li>
<li>采样（Sampling）是指对数据集进行采样的方法，包括随机采样、分层采样、过采样、欠采样等。</li>
</ul>
<h3 id="_5">数据集准备<a class="headerlink" href="#_5" title="Permanent link">&para;</a></h3>
<ul>
<li>数据集准备（Dataset Preparation）是指对数据集进行准备的方法，包括数据集的下载、数据集的解压、数据集的划分、数据集的预处理等。</li>
<li>数据集下载（Dataset Download）是指从公开数据集或第三方数据集中下载数据集的方法。</li>
<li>数据集解压（Dataset Unzip）是指解压数据集的方法。</li>
<li>数据集划分（Dataset Split）是指将数据集划分为训练集、验证集、测试集的方法。</li>
<li>数据集预处理（Dataset Preprocessing）是指对数据集进行预处理的方法，包括数据清洗、数据转换、数据归一化等。</li>
</ul>
<h3 id="_6">数据集增广<a class="headerlink" href="#_6" title="Permanent link">&para;</a></h3>
<ul>
<li>数据集增广（Dataset Augmentation）是指对数据集进行数据增广的方法，包括数据增强、数据生成等。</li>
<li>数据增强（Data Augmentation）是指通过对数据进行随机变换、扰动等方式，生成新的样本，增强数据集的规模。</li>
<li>数据生成（Data Generation）是指通过生成模型所需的数据，生成新的样本，增强数据集的规模。</li>
</ul>
<h3 id="_7">数据集采样<a class="headerlink" href="#_7" title="Permanent link">&para;</a></h3>
<ul>
<li>数据集采样（Dataset Sampling）是指对数据集进行采样的方法，包括随机采样、分层采样、过采样、欠采样等。</li>
<li>随机采样（Random Sampling）是指随机从数据集中抽取样本，生成新的样本集。</li>
<li>分层采样（Stratified Sampling）是指根据样本的类别分布，将样本按照类别比例抽取，生成新的样本集。</li>
<li>过采样（Oversampling）是指通过对少数类样本进行复制，生成新的样本集。</li>
<li>欠采样（Undersampling）是指通过对多数类样本进行删除，生成新的样本集。</li>
</ul>
<h2 id="_8">大模型训练<a class="headerlink" href="#_8" title="Permanent link">&para;</a></h2>
<p>大模型训练是指使用大模型（如 Llama3、GPT-3、BERT、RoBERTa 等）进行训练，以提升模型的训练速度。大模型训练通常需要大量的计算资源，因此，如何有效地利用这些资源，提升模型的训练速度，是提升模型性能的关键。</p>
<h3 id="_9">训练模型<a class="headerlink" href="#_9" title="Permanent link">&para;</a></h3>
<ul>
<li>模型（Model）是指机器学习中的一个概念 ，它是对输入数据进行预测或分类的算法或函数。模型的训练就是通过训练数据，使模型能够对未知数据进行预测或分类。</li>
<li>超参数搜索（Hyperparameter Search）是指通过尝试不同超参数的组合，来找到最优的超参数组合，来提升模型的训练速度。</li>
<li>多 GPU 训练（Multi-GPU Training）是指将模型分布到多个 GPU 上，并行计算，提升模型的训练速度。</li>
<li>多机多卡训练（Multi-Machine Multi-Card Training）是指将模型分布到多个机器上，并行计算，提升模型的训练速度。</li>
<li>异步数据加载（Asynchronous Data Loading）是指在模型训练时，使用多个线程或进程，异步加载数据，提升模型的训练速度。</li>
<li>预处理（Preprocessing）是指对训练数据进行预处理，生成新的训练样本，提升模型的训练速度。</li>
<li>内存优化（Memory Optimization）是指减少模型的内存占用，提升模型的训练速度。例如，减少模型的中间变量的大小，使用更小的激活函数等。
<div class="highlight"><pre><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a>- 使用更小的激活函数：使用更小的激活函数，可以减少模型的内存占用。例如，ReLU6 代替 ReLU。
<a id="__codelineno-0-2" name="__codelineno-0-2" href="#__codelineno-0-2"></a>- 使用更小的模型：使用更小的模型，可以减少模型的内存占用。例如，使用更小的卷积核、更小的滤波器等。
<a id="__codelineno-0-3" name="__codelineno-0-3" href="#__codelineno-0-3"></a>- 使用更小的 batch size：使用更小的 batch size，可以减少模型的内存占用。例如，使用更小的 batch size，可以减少内存的占用。
<a id="__codelineno-0-4" name="__codelineno-0-4" href="#__codelineno-0-4"></a>- 使用更小的学习率：使用更小的学习率，可以减少模型的内存占用。例如，使用更小的学习率，可以减少内存的占用。
<a id="__codelineno-0-5" name="__codelineno-0-5" href="#__codelineno-0-5"></a>- 使用更小的模型架构：使用更小的模型架构，可以减少模型的内存占用。例如，使用更小的模型架构，可以减少内存的占用。
<a id="__codelineno-0-6" name="__codelineno-0-6" href="#__codelineno-0-6"></a>- 使用更小的模型初始化：使用更小的模型初始化，可以减少模型的内存占用。例如，使用更小的模型初始化，可以减少内存的占用。
<a id="__codelineno-0-7" name="__codelineno-0-7" href="#__codelineno-0-7"></a>- 使用更小的模型微调：使用更小的模型微调，可以减少模型的内存占用。例如，使用更小的模型微调，可以减少内存的占用。
<a id="__codelineno-0-8" name="__codelineno-0-8" href="#__codelineno-0-8"></a>- 使用更小的模型蒸馏：使用更小的模型蒸馏，可以减少模型的内存占用。例如，使用更小的模型蒸馏，可以减少内存的占用。
<a id="__codelineno-0-9" name="__codelineno-0-9" href="#__codelineno-0-9"></a>- 使用更小的模型量化：使用更小的模型量化，可以减少模型的内存占用。例如，使用更小的模型量化，可以减少内存的占用。
<a id="__codelineno-0-10" name="__codelineno-0-10" href="#__codelineno-0-10"></a>- 使用更小的模型剪枝：使用更小的模型剪枝，可以减少模型的内存占用。例如，使用更小的模型剪枝，可以减少内存的占用。
</code></pre></div></li>
</ul>
<h3 id="_10">常用参数<a class="headerlink" href="#_10" title="Permanent link">&para;</a></h3>
<ul>
<li>学习率：学习率（Learning Rate）是模型训练时，模型参数更新的速度，是一个非常重要的超参数。控制了模型参数在每次迭代中的更新幅度，是一个非常重要的超参数。
<div class="highlight"><pre><span></span><code><a id="__codelineno-1-1" name="__codelineno-1-1" href="#__codelineno-1-1"></a>  作用：学习率决定了模型的训练速度，学习率过大或过小都会导致模型训练不稳定，收敛速度慢，模型效果不好。学习率过大可能导致震荡，学习率过小可能导致收敛速度过慢。需要根据具体情况进行调整。
<a id="__codelineno-1-2" name="__codelineno-1-2" href="#__codelineno-1-2"></a>  常用学习率：
<a id="__codelineno-1-3" name="__codelineno-1-3" href="#__codelineno-1-3"></a>  - 固定学习率：固定学习率（Fixed Learning Rate）是指模型训练时，使用固定的学习率。
<a id="__codelineno-1-4" name="__codelineno-1-4" href="#__codelineno-1-4"></a>  - 指数衰减学习率：指数衰减学习率（Exponential Decay Learning Rate）是指模型训练时，学习率随着训练轮数的增加而衰减。
<a id="__codelineno-1-5" name="__codelineno-1-5" href="#__codelineno-1-5"></a>  - 余弦退火学习率：余弦退火学习率（Cosine Annealing Learning Rate）是指模型训练时，学习率随着训练轮数的增加，在一定范围内进行线性衰减。
<a id="__codelineno-1-6" name="__codelineno-1-6" href="#__codelineno-1-6"></a>  - 余弦退火学习率：余弦退火学习率（Step Learning Rate）是指模型训练时，学习率在一定范围内进行线性衰减。
<a id="__codelineno-1-7" name="__codelineno-1-7" href="#__codelineno-1-7"></a>  - 自适应学习率：自适应学习率（Adaptive Learning Rate）是指模型训练时，学习率随着模型的训练效果而动态调整。
<a id="__codelineno-1-8" name="__codelineno-1-8" href="#__codelineno-1-8"></a>
<a id="__codelineno-1-9" name="__codelineno-1-9" href="#__codelineno-1-9"></a>  学习率示例：
<a id="__codelineno-1-10" name="__codelineno-1-10" href="#__codelineno-1-10"></a>  1、固定学习率：
<a id="__codelineno-1-11" name="__codelineno-1-11" href="#__codelineno-1-11"></a>  lr = 0.01
<a id="__codelineno-1-12" name="__codelineno-1-12" href="#__codelineno-1-12"></a>
<a id="__codelineno-1-13" name="__codelineno-1-13" href="#__codelineno-1-13"></a>  2、指数衰减学习率：
<a id="__codelineno-1-14" name="__codelineno-1-14" href="#__codelineno-1-14"></a>  lr = 0.01
<a id="__codelineno-1-15" name="__codelineno-1-15" href="#__codelineno-1-15"></a>  lr_decay = 0.9
<a id="__codelineno-1-16" name="__codelineno-1-16" href="#__codelineno-1-16"></a>  lr_min = 0.0001
<a id="__codelineno-1-17" name="__codelineno-1-17" href="#__codelineno-1-17"></a>  lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=lr_decay, last_epoch=-1, verbose=False)
<a id="__codelineno-1-18" name="__codelineno-1-18" href="#__codelineno-1-18"></a>
<a id="__codelineno-1-19" name="__codelineno-1-19" href="#__codelineno-1-19"></a>  3、余弦退火学习率：
<a id="__codelineno-1-20" name="__codelineno-1-20" href="#__codelineno-1-20"></a>  lr = 0.01
<a id="__codelineno-1-21" name="__codelineno-1-21" href="#__codelineno-1-21"></a>  T_max = 10
<a id="__codelineno-1-22" name="__codelineno-1-22" href="#__codelineno-1-22"></a>  lr_min = 0.0001
<a id="__codelineno-1-23" name="__codelineno-1-23" href="#__codelineno-1-23"></a>  lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max, eta_min=lr_min, last_epoch=-1, verbose=False)
<a id="__codelineno-1-24" name="__codelineno-1-24" href="#__codelineno-1-24"></a>
<a id="__codelineno-1-25" name="__codelineno-1-25" href="#__codelineno-1-25"></a>  4、余弦退火学习率：
<a id="__codelineno-1-26" name="__codelineno-1-26" href="#__codelineno-1-26"></a>  lr = 0.01
<a id="__codelineno-1-27" name="__codelineno-1-27" href="#__codelineno-1-27"></a>  T_max = 10
<a id="__codelineno-1-28" name="__codelineno-1-28" href="#__codelineno-1-28"></a>  lr_min = 0.0001
<a id="__codelineno-1-29" name="__codelineno-1-29" href="#__codelineno-1-29"></a>  lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1, last_epoch=-1, verbose=False)
<a id="__codelineno-1-30" name="__codelineno-1-30" href="#__codelineno-1-30"></a>
<a id="__codelineno-1-31" name="__codelineno-1-31" href="#__codelineno-1-31"></a>  5、自适应学习率：
<a id="__codelineno-1-32" name="__codelineno-1-32" href="#__codelineno-1-32"></a>  lr = 0.01
<a id="__codelineno-1-33" name="__codelineno-1-33" href="#__codelineno-1-33"></a>  lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=&#39;min&#39;, factor=0.1, patience=5, verbose=False)
</code></pre></div></li>
<li>
<p>优化器：优化器（Optimizer）是指模型训练时，更新模型参数的算法。如随机梯度下降（SGD）、Adam、RMSprop等。
<div class="highlight"><pre><span></span><code><a id="__codelineno-2-1" name="__codelineno-2-1" href="#__codelineno-2-1"></a>  作用：优化器决定了模型参数更新的方向，优化器的选择会影响模型的训练效果。
<a id="__codelineno-2-2" name="__codelineno-2-2" href="#__codelineno-2-2"></a>  常用优化器：
<a id="__codelineno-2-3" name="__codelineno-2-3" href="#__codelineno-2-3"></a>  - 动量优化器：动量优化器（Momentum Optimizer）是指模型训练时，使用动量（Momentum）来加速模型参数的更新。如 SGD、NAG、RMSprop 等。
<a id="__codelineno-2-4" name="__codelineno-2-4" href="#__codelineno-2-4"></a>  - 动量加权平均优化器：动量加权平均优化器（Nesterov Accelerated Gradient Descent）是指模型训练时，使用动量（Momentum）和加权平均（Nesterov）来加速模型参数的更新。如 SGD、NAG、RMSprop 等。
<a id="__codelineno-2-5" name="__codelineno-2-5" href="#__codelineno-2-5"></a>  - 自适应梯度优化器：自适应梯度优化器（Adagrad Optimizer）是指模型训练时，使用自适应梯度（Adagrad）来动态调整学习率。如 Adagrad、Adadelta、Adam 等。
<a id="__codelineno-2-6" name="__codelineno-2-6" href="#__codelineno-2-6"></a>  - 自适应矩形法优化器：自适应矩形法优化器（Adadelta Optimizer）是指模型训练时，使用自适应矩形法（Adadelta）来动态调整学习率。如 Adadelta、Adagrad、Adam 等。
<a id="__codelineno-2-7" name="__codelineno-2-7" href="#__codelineno-2-7"></a>  - RMSprop 优化器：RMSprop 优化器（RMSprop Optimizer）是指模型训练时，使用 RMSprop 来动态调整学习率。如 RMSprop、Adagrad、Adam 等。
<a id="__codelineno-2-8" name="__codelineno-2-8" href="#__codelineno-2-8"></a>  - Adam 优化器：Adam 优化器（Adam Optimizer）是指模型训练时，使用 Adam 来动态调整学习率。如 Adam、Adagrad、Adadelta 等。
<a id="__codelineno-2-9" name="__codelineno-2-9" href="#__codelineno-2-9"></a>  - AdamW 优化器：带权重衰减的自适应矩估计优化器（AdamW Optimizer）是指模型训练时，使用带权重衰减的自适应矩估计（AdamW）来动态调整学习率。如 AdamW、Adam、Adagrad 等。
<a id="__codelineno-2-10" name="__codelineno-2-10" href="#__codelineno-2-10"></a>  - LBFGS 优化器：局部最优优化器（LBFGS Optimizer）是指模型训练时，使用局部最优优化（LBFGS）来动态调整学习率。
<a id="__codelineno-2-11" name="__codelineno-2-11" href="#__codelineno-2-11"></a>  - Rprop 优化器：修正的随机梯度下降优化器（Rprop Optimizer）是指模型训练时，使用修正的随机梯度下降（Rprop）来动态调整学习率。
<a id="__codelineno-2-12" name="__codelineno-2-12" href="#__codelineno-2-12"></a>  - AdamP 优化器：带权重惩罚的自适应矩估计优化器（AdamP Optimizer）是指模型训练时，使用带权重惩罚的自适应矩估计（AdamP）来动态调整学习率。
<a id="__codelineno-2-13" name="__codelineno-2-13" href="#__codelineno-2-13"></a>  - 其他优化器：其他优化器（Other Optimizer）是指模型训练时，使用其他优化器来优化模型参数。
<a id="__codelineno-2-14" name="__codelineno-2-14" href="#__codelineno-2-14"></a>
<a id="__codelineno-2-15" name="__codelineno-2-15" href="#__codelineno-2-15"></a>  优化器示例：
<a id="__codelineno-2-16" name="__codelineno-2-16" href="#__codelineno-2-16"></a>  1、动量优化器：
<a id="__codelineno-2-17" name="__codelineno-2-17" href="#__codelineno-2-17"></a>  optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)
<a id="__codelineno-2-18" name="__codelineno-2-18" href="#__codelineno-2-18"></a>
<a id="__codelineno-2-19" name="__codelineno-2-19" href="#__codelineno-2-19"></a>  2、动量加权平均优化器：
<a id="__codelineno-2-20" name="__codelineno-2-20" href="#__codelineno-2-20"></a>  optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9, nesterov=True)
<a id="__codelineno-2-21" name="__codelineno-2-21" href="#__codelineno-2-21"></a>
<a id="__codelineno-2-22" name="__codelineno-2-22" href="#__codelineno-2-22"></a>  3、自适应梯度优化器：
<a id="__codelineno-2-23" name="__codelineno-2-23" href="#__codelineno-2-23"></a>  optimizer = torch.optim.Adagrad(model.parameters(), lr=0.01, lr_decay=0.001)
<a id="__codelineno-2-24" name="__codelineno-2-24" href="#__codelineno-2-24"></a>
<a id="__codelineno-2-25" name="__codelineno-2-25" href="#__codelineno-2-25"></a>  4、自适应矩形法优化器：
<a id="__codelineno-2-26" name="__codelineno-2-26" href="#__codelineno-2-26"></a>  optimizer = torch.optim.Adadelta(model.parameters(), lr=1.0, rho=0.9, eps=1e-06, weight_decay=0)
<a id="__codelineno-2-27" name="__codelineno-2-27" href="#__codelineno-2-27"></a>
<a id="__codelineno-2-28" name="__codelineno-2-28" href="#__codelineno-2-28"></a>  5、RMSprop 优化器：
<a id="__codelineno-2-29" name="__codelineno-2-29" href="#__codelineno-2-29"></a>  optimizer = torch.optim.RMSprop(model.parameters(), lr=0.01, alpha=0.99, eps=1e-08, weight_decay=0, momentum=0, centered=False)
<a id="__codelineno-2-30" name="__codelineno-2-30" href="#__codelineno-2-30"></a>
<a id="__codelineno-2-31" name="__codelineno-2-31" href="#__codelineno-2-31"></a>  6、Adam 优化器：
<a id="__codelineno-2-32" name="__codelineno-2-32" href="#__codelineno-2-32"></a>  optimizer = torch.optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)
</code></pre></div></p>
</li>
<li>
<p>批大小：批大小（Batch Size）是指模型训练时，一次处理的数据量。每次迭代更新模型参数时所使用的样本数量。
<div class="highlight"><pre><span></span><code><a id="__codelineno-3-1" name="__codelineno-3-1" href="#__codelineno-3-1"></a>  作用：批量大小决定了模型训练时的内存占用，批量大小过大或过小都会导致模型训练不稳定，甚至收敛速度慢。影响模型参数更新的频率和计算效率。通常，较大的批量大小可以提高计算效率，但可能会导致模型陷入局部最小值。较小的批量大小可以减少内存占用，但可能会导致模型收敛速度慢。
<a id="__codelineno-3-2" name="__codelineno-3-2" href="#__codelineno-3-2"></a>  常用批量大小：
<a id="__codelineno-3-3" name="__codelineno-3-3" href="#__codelineno-3-3"></a>  - 固定批量大小：固定批量大小（Fixed Batch Size）是指模型训练时，使用固定的批量大小。
<a id="__codelineno-3-4" name="__codelineno-3-4" href="#__codelineno-3-4"></a>  - 自适应批量大小：自适应批量大小（Adaptive Batch Size）是指模型训练时，批量大小随着模型的训练效果而动态调整。
<a id="__codelineno-3-5" name="__codelineno-3-5" href="#__codelineno-3-5"></a>
<a id="__codelineno-3-6" name="__codelineno-3-6" href="#__codelineno-3-6"></a>  批大小示例：
<a id="__codelineno-3-7" name="__codelineno-3-7" href="#__codelineno-3-7"></a>  1、固定批量大小：
<a id="__codelineno-3-8" name="__codelineno-3-8" href="#__codelineno-3-8"></a>  batch_size = 128
<a id="__codelineno-3-9" name="__codelineno-3-9" href="#__codelineno-3-9"></a>
<a id="__codelineno-3-10" name="__codelineno-3-10" href="#__codelineno-3-10"></a>  2、自适应批量大小：
<a id="__codelineno-3-11" name="__codelineno-3-11" href="#__codelineno-3-11"></a>  batch_size = 128
<a id="__codelineno-3-12" name="__codelineno-3-12" href="#__codelineno-3-12"></a>  batch_size_min = 32
<a id="__codelineno-3-13" name="__codelineno-3-13" href="#__codelineno-3-13"></a>  batch_size_max = 256
<a id="__codelineno-3-14" name="__codelineno-3-14" href="#__codelineno-3-14"></a>  batch_size_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=&#39;min&#39;, factor=0.1, patience=5, verbose=False)
</code></pre></div></p>
</li>
<li>
<p>迭代次数/训练轮数：轮数（Epoch）是指模型训练时，模型训练的次数。指定训练过程中数据集被完整遍历的次数。一个 epoch 代表了对整个数据集的一次完整训练。
<div class="highlight"><pre><span></span><code><a id="__codelineno-4-1" name="__codelineno-4-1" href="#__codelineno-4-1"></a>  作用：训练轮数决定了模型的训练时间，训练轮数过多或过少都会导致模型训练不稳定，甚至收敛速度慢。控制训练过程的时长，需要根据模型和数据集的复杂程度进行调整。
<a id="__codelineno-4-2" name="__codelineno-4-2" href="#__codelineno-4-2"></a>  常用训练轮数：
<a id="__codelineno-4-3" name="__codelineno-4-3" href="#__codelineno-4-3"></a>  - 固定训练轮数：固定训练轮数（Fixed Number of Epochs）是指模型训练时，使用固定的训练轮数。
<a id="__codelineno-4-4" name="__codelineno-4-4" href="#__codelineno-4-4"></a>  - 自适应训练轮数：自适应训练轮数（Adaptive Number of Epochs）是指模型训练时，训练轮数随着模型的训练效果而动态调整。
<a id="__codelineno-4-5" name="__codelineno-4-5" href="#__codelineno-4-5"></a>
<a id="__codelineno-4-6" name="__codelineno-4-6" href="#__codelineno-4-6"></a>  训练轮数示例：
<a id="__codelineno-4-7" name="__codelineno-4-7" href="#__codelineno-4-7"></a>  1、固定训练轮数：
<a id="__codelineno-4-8" name="__codelineno-4-8" href="#__codelineno-4-8"></a>  num_epochs = 10
<a id="__codelineno-4-9" name="__codelineno-4-9" href="#__codelineno-4-9"></a>
<a id="__codelineno-4-10" name="__codelineno-4-10" href="#__codelineno-4-10"></a>  2、自适应训练轮数：
<a id="__codelineno-4-11" name="__codelineno-4-11" href="#__codelineno-4-11"></a>  num_epochs = 10
<a id="__codelineno-4-12" name="__codelineno-4-12" href="#__codelineno-4-12"></a>  num_epochs_min = 5
<a id="__codelineno-4-13" name="__codelineno-4-13" href="#__codelineno-4-13"></a>  num_epochs_max = 20
<a id="__codelineno-4-14" name="__codelineno-4-14" href="#__codelineno-4-14"></a>  num_epochs_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=&#39;min&#39;, factor=0.1, patience=5, verbose=False)
</code></pre></div></p>
</li>
<li>
<p>损失函数：损失函数（Loss Function）是指模型训练的目标函数，用于衡量模型的预测值与真实值之间的差距。
<div class="highlight"><pre><span></span><code><a id="__codelineno-5-1" name="__codelineno-5-1" href="#__codelineno-5-1"></a>  描述：用于衡量模型预测与真实标签之间的差异，不同任务和模型可能需要选择不同的损失函数，如均方误差（MSE）、交叉熵等。
<a id="__codelineno-5-2" name="__codelineno-5-2" href="#__codelineno-5-2"></a>  常用损失函数：
<a id="__codelineno-5-3" name="__codelineno-5-3" href="#__codelineno-5-3"></a>  - 均方误差（MSE）：均方误差（Mean Squared Error）是指预测值与真实值之间的差距的平方。
<a id="__codelineno-5-4" name="__codelineno-5-4" href="#__codelineno-5-4"></a>  - 交叉熵（Cross Entropy）：交叉熵（Cross-Entropy）是指预测值与真实值之间的差距的对数。
<a id="__codelineno-5-5" name="__codelineno-5-5" href="#__codelineno-5-5"></a>  - 分类损失函数：分类损失函数（Classification Loss Function）是指用于分类任务的损失函数，如交叉熵、对数损失、KL 散度等。
<a id="__codelineno-5-6" name="__codelineno-5-6" href="#__codelineno-5-6"></a>  - 回归损失函数：回归损失函数（Regression Loss Function）是指用于回归任务的损失函数，如均方误差、Huber 损失、绝对损失等。
<a id="__codelineno-5-7" name="__codelineno-5-7" href="#__codelineno-5-7"></a>  - 多标签分类损失函数：多标签分类损失函数（Multi-Label Classification Loss Function）是指用于多标签分类任务的损失函数，如 F1 损失、多标签分类交叉熵等。
<a id="__codelineno-5-8" name="__codelineno-5-8" href="#__codelineno-5-8"></a>  - 多任务损失函数：多任务损失函数（Multi-Task Loss Function）是指用于多任务学习任务的损失函数，如联合损失、分割损失、对比损失等。
<a id="__codelineno-5-9" name="__codelineno-5-9" href="#__codelineno-5-9"></a>  - 标签平滑损失函数：标签平滑损失函数（Label Smoothing Loss Function）是指用于处理标签平滑问题的损失函数，如平滑交叉熵、标签平滑损失等。
<a id="__codelineno-5-10" name="__codelineno-5-10" href="#__codelineno-5-10"></a>  - 多分类损失函数：多分类损失函数（Multi-Class Loss Function）是指用于多分类任务的损失函数，如 Focal 损失、多分类交叉熵等。
<a id="__codelineno-5-11" name="__codelineno-5-11" href="#__codelineno-5-11"></a>  - 多尺度损失函数：多尺度损失函数（Multi-Scale Loss Function）是指用于多尺度学习任务的损失函数，如 FPN 损失、多尺度损失等。
<a id="__codelineno-5-12" name="__codelineno-5-12" href="#__codelineno-5-12"></a>  - 多样本损失函数：多样本损失函数（Multi-Sample Loss Function）是指用于多样本学习任务的损失函数，如 NCE 损失、多样本损失等。
<a id="__codelineno-5-13" name="__codelineno-5-13" href="#__codelineno-5-13"></a>  - 自监督损失函数：自监督损失函数（Self-Supervised Loss Function）是指用于自监督学习任务的损失函数，如对比损失、自监督学习损失等。
<a id="__codelineno-5-14" name="__codelineno-5-14" href="#__codelineno-5-14"></a>  - 序列到序列损失函数：序列到序列损失函数（Sequence-to-Sequence Loss Function）是指用于序列到序列学习任务的损失函数，如注意力损失、CTC 损失等。
<a id="__codelineno-5-15" name="__codelineno-5-15" href="#__codelineno-5-15"></a>  - 视频序列损失函数：视频序列损失函数（Video Sequence Loss Function）是指用于视频序列学习任务的损失函数，如视频损失、时序损失等。
<a id="__codelineno-5-16" name="__codelineno-5-16" href="#__codelineno-5-16"></a>  - 强化学习损失函数：强化学习损失函数（Reinforcement Learning Loss Function）是指用于强化学习任务的损失函数，如策略梯度损失、策略梯度更新损失等。
<a id="__codelineno-5-17" name="__codelineno-5-17" href="#__codelineno-5-17"></a>  - 其他损失函数：其他损失函数（Other Loss Function）是指用于其他任务的损失函数，如多任务损失、多尺度损失、多样本损失等。
<a id="__codelineno-5-18" name="__codelineno-5-18" href="#__codelineno-5-18"></a>
<a id="__codelineno-5-19" name="__codelineno-5-19" href="#__codelineno-5-19"></a>  损失函数示例：
<a id="__codelineno-5-20" name="__codelineno-5-20" href="#__codelineno-5-20"></a>  1、均方误差（MSE）：
<a id="__codelineno-5-21" name="__codelineno-5-21" href="#__codelineno-5-21"></a>  import torch.nn as nn
<a id="__codelineno-5-22" name="__codelineno-5-22" href="#__codelineno-5-22"></a>  loss_fn = nn.MSELoss()
<a id="__codelineno-5-23" name="__codelineno-5-23" href="#__codelineno-5-23"></a>
<a id="__codelineno-5-24" name="__codelineno-5-24" href="#__codelineno-5-24"></a>  2、交叉熵（Cross Entropy）：
<a id="__codelineno-5-25" name="__codelineno-5-25" href="#__codelineno-5-25"></a>  import torch.nn as nn
<a id="__codelineno-5-26" name="__codelineno-5-26" href="#__codelineno-5-26"></a>  loss_fn = nn.CrossEntropyLoss()
<a id="__codelineno-5-27" name="__codelineno-5-27" href="#__codelineno-5-27"></a>
<a id="__codelineno-5-28" name="__codelineno-5-28" href="#__codelineno-5-28"></a>  3、分类损失函数：
<a id="__codelineno-5-29" name="__codelineno-5-29" href="#__codelineno-5-29"></a>  import torch.nn as nn
<a id="__codelineno-5-30" name="__codelineno-5-30" href="#__codelineno-5-30"></a>  loss_fn = nn.BCEWithLogitsLoss()
<a id="__codelineno-5-31" name="__codelineno-5-31" href="#__codelineno-5-31"></a>
<a id="__codelineno-5-32" name="__codelineno-5-32" href="#__codelineno-5-32"></a>  4、回归损失函数：
<a id="__codelineno-5-33" name="__codelineno-5-33" href="#__codelineno-5-33"></a>  import torch.nn as nn
<a id="__codelineno-5-34" name="__codelineno-5-34" href="#__codelineno-5-34"></a>  loss_fn = nn.L1Loss()
<a id="__codelineno-5-35" name="__codelineno-5-35" href="#__codelineno-5-35"></a>
<a id="__codelineno-5-36" name="__codelineno-5-36" href="#__codelineno-5-36"></a>  5、多标签分类损失函数：
<a id="__codelineno-5-37" name="__codelineno-5-37" href="#__codelineno-5-37"></a>  import torch.nn as nn
<a id="__codelineno-5-38" name="__codelineno-5-38" href="#__codelineno-5-38"></a>  loss_fn = nn.BCEWithLogitsLoss()
<a id="__codelineno-5-39" name="__codelineno-5-39" href="#__codelineno-5-39"></a>
<a id="__codelineno-5-40" name="__codelineno-5-40" href="#__codelineno-5-40"></a>  6、多任务损失函数：
<a id="__codelineno-5-41" name="__codelineno-5-41" href="#__codelineno-5-41"></a>  import torch.nn as nn
<a id="__codelineno-5-42" name="__codelineno-5-42" href="#__codelineno-5-42"></a>  loss_fn = nn.MultiTaskLoss()
<a id="__codelineno-5-43" name="__codelineno-5-43" href="#__codelineno-5-43"></a>
<a id="__codelineno-5-44" name="__codelineno-5-44" href="#__codelineno-5-44"></a>  7、标签平滑损失函数：
<a id="__codelineno-5-45" name="__codelineno-5-45" href="#__codelineno-5-45"></a>  import torch.nn as nn
<a id="__codelineno-5-46" name="__codelineno-5-46" href="#__codelineno-5-46"></a>  loss_fn = nn.KLDivLoss()
<a id="__codelineno-5-47" name="__codelineno-5-47" href="#__codelineno-5-47"></a>
<a id="__codelineno-5-48" name="__codelineno-5-48" href="#__codelineno-5-48"></a>  8、多分类损失函数：
<a id="__codelineno-5-49" name="__codelineno-5-49" href="#__codelineno-5-49"></a>  import torch.nn as nn
<a id="__codelineno-5-50" name="__codelineno-5-50" href="#__codelineno-5-50"></a>  loss_fn = nn.CrossEntropyLoss()
<a id="__codelineno-5-51" name="__codelineno-5-51" href="#__codelineno-5-51"></a>
<a id="__codelineno-5-52" name="__codelineno-5-52" href="#__codelineno-5-52"></a>  9、多尺度损失函数：
<a id="__codelineno-5-53" name="__codelineno-5-53" href="#__codelineno-5-53"></a>  import torch.nn as nn
<a id="__codelineno-5-54" name="__codelineno-5-54" href="#__codelineno-5-54"></a>  loss_fn = nn.MultiScaleLoss()
<a id="__codelineno-5-55" name="__codelineno-5-55" href="#__codelineno-5-55"></a>
<a id="__codelineno-5-56" name="__codelineno-5-56" href="#__codelineno-5-56"></a>  10、多样本损失函数：
<a id="__codelineno-5-57" name="__codelineno-5-57" href="#__codelineno-5-57"></a>  import torch.nn as nn
<a id="__codelineno-5-58" name="__codelineno-5-58" href="#__codelineno-5-58"></a>  loss_fn = nn.NLLLoss()
<a id="__codelineno-5-59" name="__codelineno-5-59" href="#__codelineno-5-59"></a>
<a id="__codelineno-5-60" name="__codelineno-5-60" href="#__codelineno-5-60"></a>  11、自监督损失函数：
<a id="__codelineno-5-61" name="__codelineno-5-61" href="#__codelineno-5-61"></a>  import torch.nn as nn
<a id="__codelineno-5-62" name="__codelineno-5-62" href="#__codelineno-5-62"></a>  loss_fn = nn.SupervisedLoss()
<a id="__codelineno-5-63" name="__codelineno-5-63" href="#__codelineno-5-63"></a>
<a id="__codelineno-5-64" name="__codelineno-5-64" href="#__codelineno-5-64"></a>  12、序列到序列损失函数：
<a id="__codelineno-5-65" name="__codelineno-5-65" href="#__codelineno-5-65"></a>  import torch.nn as nn
<a id="__codelineno-5-66" name="__codelineno-5-66" href="#__codelineno-5-66"></a>  loss_fn = nn.CTCLoss()
<a id="__codelineno-5-67" name="__codelineno-5-67" href="#__codelineno-5-67"></a>
<a id="__codelineno-5-68" name="__codelineno-5-68" href="#__codelineno-5-68"></a>  13、视频序列损失函数：
<a id="__codelineno-5-69" name="__codelineno-5-69" href="#__codelineno-5-69"></a>  import torch.nn as nn
<a id="__codelineno-5-70" name="__codelineno-5-70" href="#__codelineno-5-70"></a>  loss_fn = nn.VideoLoss()
<a id="__codelineno-5-71" name="__codelineno-5-71" href="#__codelineno-5-71"></a>
<a id="__codelineno-5-72" name="__codelineno-5-72" href="#__codelineno-5-72"></a>  14、强化学习损失函数：
<a id="__codelineno-5-73" name="__codelineno-5-73" href="#__codelineno-5-73"></a>  import torch.nn as nn
<a id="__codelineno-5-74" name="__codelineno-5-74" href="#__codelineno-5-74"></a>  loss_fn = nn.PolicyGradientLoss()
<a id="__codelineno-5-75" name="__codelineno-5-75" href="#__codelineno-5-75"></a>
<a id="__codelineno-5-76" name="__codelineno-5-76" href="#__codelineno-5-76"></a>  15、其他损失函数：
<a id="__codelineno-5-77" name="__codelineno-5-77" href="#__codelineno-5-77"></a>  import torch.nn as nn
<a id="__codelineno-5-78" name="__codelineno-5-78" href="#__codelineno-5-78"></a>  loss_fn = nn.Loss()
</code></pre></div></p>
</li>
<li>
<p>正则化：正则化（Regularization）是指模型训练时，对模型参数进行约束，以防止模型过拟合。
<div class="highlight"><pre><span></span><code><a id="__codelineno-6-1" name="__codelineno-6-1" href="#__codelineno-6-1"></a>作用：防止模型过拟合，提升模型的泛化能力。
<a id="__codelineno-6-2" name="__codelineno-6-2" href="#__codelineno-6-2"></a>常用正则化：
<a id="__codelineno-6-3" name="__codelineno-6-3" href="#__codelineno-6-3"></a>- L1 正则化：L1 正则化（Lasso Regularization）是指模型训练时，对模型参数进行惩罚，使得模型参数的绝对值之和（L1 范数）小于某个阈值。
<a id="__codelineno-6-4" name="__codelineno-6-4" href="#__codelineno-6-4"></a>- L2 正则化：L2 正则化（Ridge Regularization）是指模型训练时，对模型参数进行惩罚，使得模型参数的平方之和（L2 范数）小于某个阈值。
<a id="__codelineno-6-5" name="__codelineno-6-5" href="#__codelineno-6-5"></a>- 弹性网络：弹性网络（Elastic Net）是指模型训练时，同时使用 L1 正则化和 L2 正则化。
<a id="__codelineno-6-6" name="__codelineno-6-6" href="#__codelineno-6-6"></a>- 最大熵正则化：最大熵正则化（Max Entropy Regularization）是指模型训练时，对模型参数进行惩罚，使得模型参数满足最大熵原理。
<a id="__codelineno-6-7" name="__codelineno-6-7" href="#__codelineno-6-7"></a>- 丢弃法：丢弃法（Dropout）是指模型训练时，随机将一部分神经元的输出设置为 0，以减少模型过拟合。
<a id="__codelineno-6-8" name="__codelineno-6-8" href="#__codelineno-6-8"></a>- 最大池化：最大池化（Max Pooling）是指模型训练时，对模型参数进行约束，使得模型参数的最大值不超过某个阈值。
<a id="__codelineno-6-9" name="__codelineno-6-9" href="#__codelineno-6-9"></a>- 最小池化：最小池化（Min Pooling）是指模型训练时，对模型参数进行约束，使得模型参数的最小值不小于某个阈值。
<a id="__codelineno-6-10" name="__codelineno-6-10" href="#__codelineno-6-10"></a>- 局部响应归一化：局部响应归一化（Local Response Normalization）是指模型训练时，对模型参数进行约束，使得模型参数的局部响应值（局部激活）不小于某个阈值。
<a id="__codelineno-6-11" name="__codelineno-6-11" href="#__codelineno-6-11"></a>- 归一化层：归一化层（Normalization Layer）是指模型训练时，对模型参数进行约束，使得模型参数的均值和方差不变。
<a id="__codelineno-6-12" name="__codelineno-6-12" href="#__codelineno-6-12"></a>- 标签平滑：标签平滑（Label Smoothing）是指模型训练时，对模型标签进行平滑处理，使得模型的训练更加稳定。
<a id="__codelineno-6-13" name="__codelineno-6-13" href="#__codelineno-6-13"></a>- 其他正则化：其他正则化（Other Regularization）是指模型训练时，对模型参数进行其他约束，如限制模型参数的范数、限制模型参数的范围等。
</code></pre></div></p>
</li>
<li>学习率衰减：学习率衰减（Learning Rate Decay）是指模型训练时，学习率随着训练轮数的增加而衰减。
<div class="highlight"><pre><span></span><code><a id="__codelineno-7-1" name="__codelineno-7-1" href="#__codelineno-7-1"></a>作用：防止模型过拟合，提升模型的泛化能力。
<a id="__codelineno-7-2" name="__codelineno-7-2" href="#__codelineno-7-2"></a>常用学习率衰减：
<a id="__codelineno-7-3" name="__codelineno-7-3" href="#__codelineno-7-3"></a>- 指数衰减：指数衰减（Exponential Decay）是指学习率随着训练轮数的增加，逐渐衰减。
<a id="__codelineno-7-4" name="__codelineno-7-4" href="#__codelineno-7-4"></a>- 余弦衰减：余弦衰减（Cosine Decay）是指学习率随着训练轮数的增加，在一定范围内，学习率以余弦函数的形式衰减。
<a id="__codelineno-7-5" name="__codelineno-7-5" href="#__codelineno-7-5"></a>- 线性衰减：线性衰减（Linear Decay）是指学习率随着训练轮数的增加，逐渐衰减。
<a id="__codelineno-7-6" name="__codelineno-7-6" href="#__codelineno-7-6"></a>- 多项式衰减：多项式衰减（Polynomial Decay）是指学习率随着训练轮数的增加，以多项式函数的形式衰减。
<a id="__codelineno-7-7" name="__codelineno-7-7" href="#__codelineno-7-7"></a>- 其他学习率衰减：其他学习率衰减（Other Learning Rate Decay）是指学习率随着训练轮数的增加，以其他方式衰减。
</code></pre></div></li>
<li>
<p>模型大小：模型大小（Model Size）是指模型的计算量。
<div class="highlight"><pre><span></span><code><a id="__codelineno-8-1" name="__codelineno-8-1" href="#__codelineno-8-1"></a>作用：影响模型训练的速度和内存占用。
<a id="__codelineno-8-2" name="__codelineno-8-2" href="#__codelineno-8-2"></a>常用模型大小：
<a id="__codelineno-8-3" name="__codelineno-8-3" href="#__codelineno-8-3"></a>- 小模型：小模型（Small Model）是指模型的计算量较小，适合于资源有限的场景。
<a id="__codelineno-8-4" name="__codelineno-8-4" href="#__codelineno-8-4"></a>- 中型模型：中型模型（Medium Model）是指模型的计算量适中，适合于中等规模的场景。
<a id="__codelineno-8-5" name="__codelineno-8-5" href="#__codelineno-8-5"></a>- 大型模型：大型模型（Large Model）是指模型的计算量较大，适合于大规模的场景。
<a id="__codelineno-8-6" name="__codelineno-8-6" href="#__codelineno-8-6"></a>- 超大型模型：超大型模型（Huge Model）是指模型的计算量非常大，适合于超大规模的场景。
</code></pre></div></p>
</li>
<li>
<p>模型初始化：模型初始化（Model Initialization）是指模型参数的初始值，用于控制模型的训练起点。
<div class="highlight"><pre><span></span><code><a id="__codelineno-9-1" name="__codelineno-9-1" href="#__codelineno-9-1"></a>作用：控制模型训练的起点。
<a id="__codelineno-9-2" name="__codelineno-9-2" href="#__codelineno-9-2"></a>常用模型初始化：
<a id="__codelineno-9-3" name="__codelineno-9-3" href="#__codelineno-9-3"></a>- 随机初始化：随机初始化（Random Initialization）是指模型参数的初始值随机生成。
<a id="__codelineno-9-4" name="__codelineno-9-4" href="#__codelineno-9-4"></a>- 预训练初始化：预训练初始化（Pre-trained Initialization）是指模型参数的初始值使用预训练模型的权重。
<a id="__codelineno-9-5" name="__codelineno-9-5" href="#__codelineno-9-5"></a>- 其他初始化：其他初始化（Other Initialization）是指模型参数的初始值使用其他方式。
</code></pre></div></p>
</li>
<li>
<p>初始化方法：初始化方法（Initialization Method）是指模型参数的初始值生成方法。设置模型参数的初始值，如随机初始化、Xavier/Glorot 初始化等。
<div class="highlight"><pre><span></span><code><a id="__codelineno-10-1" name="__codelineno-10-1" href="#__codelineno-10-1"></a>作用：控制模型参数的初始值生成方法。影响模型训练的初始状态，有助于避免模型陷入局部最小值。
<a id="__codelineno-10-2" name="__codelineno-10-2" href="#__codelineno-10-2"></a>常用初始化方法：
<a id="__codelineno-10-3" name="__codelineno-10-3" href="#__codelineno-10-3"></a>- 常数初始化：常数初始化（Constant Initialization）是指模型参数的初始值全部设置为某个常数。
<a id="__codelineno-10-4" name="__codelineno-10-4" href="#__codelineno-10-4"></a>- 正态分布初始化：正态分布初始化（Normal Initialization）是指模型参数的初始值服从正态分布。
<a id="__codelineno-10-5" name="__codelineno-10-5" href="#__codelineno-10-5"></a>- 均匀分布初始化：均匀分布初始化（Uniform Initialization）是指模型参数的初始值服从均匀分布。
<a id="__codelineno-10-6" name="__codelineno-10-6" href="#__codelineno-10-6"></a>- Xavier 正态分布初始化：Xavier 正态分布初始化（Xavier Normal Initialization）是指模型参数的初始值服从 Xavier 正态分布。
<a id="__codelineno-10-7" name="__codelineno-10-7" href="#__codelineno-10-7"></a>- Kaiming 正态分布初始化：Kaiming 正态分布初始化（Kaiming Normal Initialization）是指模型参数的初始值服从 Kaiming 正态分布。
<a id="__codelineno-10-8" name="__codelineno-10-8" href="#__codelineno-10-8"></a>- 其他初始化方法：其他初始化方法（Other Initialization Method）是指模型参数的初始值生成方法。
</code></pre></div></p>
</li>
<li>
<p>验证集：验证集（Validation Set）是指模型训练时，使用一部分训练数据留出作为验证集，用于调参和评估模型性能。
<div class="highlight"><pre><span></span><code><a id="__codelineno-11-1" name="__codelineno-11-1" href="#__codelineno-11-1"></a>作用：帮助选择最佳的超参数，避免模型在测试集上过拟合。
</code></pre></div></p>
</li>
<li>超参数：超参数（Hyperparameter）是指模型训练时，需要设置的参数，如学习率、权重衰减、模型大小、优化器等。
<div class="highlight"><pre><span></span><code><a id="__codelineno-12-1" name="__codelineno-12-1" href="#__codelineno-12-1"></a>作用：控制模型训练的超参数，影响模型的训练过程。
<a id="__codelineno-12-2" name="__codelineno-12-2" href="#__codelineno-12-2"></a>- 超参数搜索（Hyperparameter Search）是指模型训练时，通过搜索超参数的组合，来优化模型的性能。
<a id="__codelineno-12-3" name="__codelineno-12-3" href="#__codelineno-12-3"></a>- 随机搜索（Random Search）是指模型训练时，通过随机搜索超参数的组合，来优化模型的性能。
<a id="__codelineno-12-4" name="__codelineno-12-4" href="#__codelineno-12-4"></a>- 贝叶斯优化（Bayesian Optimization）是指模型训练时，通过贝叶斯优化超参数的组合，来优化模型的性能。
<a id="__codelineno-12-5" name="__codelineno-12-5" href="#__codelineno-12-5"></a>- 遗传算法（Genetic Algorithm）是指模型训练时，通过遗传算法搜索超参数的组合，来优化模型的性能。
<a id="__codelineno-12-6" name="__codelineno-12-6" href="#__codelineno-12-6"></a>- 启发式搜索（Heuristic Search）是指模型训练时，通过启发式搜索超参数的组合，来优化模型的性能。
<a id="__codelineno-12-7" name="__codelineno-12-7" href="#__codelineno-12-7"></a>- 其他超参数优化方法：其他超参数优化方法（Other Hyperparameter Optimization Method）是指模型训练时，用于优化超参数的其他方法。
</code></pre></div></li>
<li>其他技巧：其他技巧（Other Tips）是指模型训练时，其他技巧，如模型蒸馏、模型微调等。
<div class="highlight"><pre><span></span><code><a id="__codelineno-13-1" name="__codelineno-13-1" href="#__codelineno-13-1"></a>- 批归一化：批归一化（Batch Normalization）是指模型训练时，对输入数据进行归一化处理，使得数据在各层之间具有相同的分布。
<a id="__codelineno-13-2" name="__codelineno-13-2" href="#__codelineno-13-2"></a>- 动量：动量（Momentum）是指模型训练时，梯度下降时，对梯度的更新方向进行加权，以加快模型的收敛速度。
<a id="__codelineno-13-3" name="__codelineno-13-3" href="#__codelineno-13-3"></a>- 模型微调：模型微调（Model Fine-tuning）是指在已有模型的基础上，微调模型参数，提升模型的性能。
<a id="__codelineno-13-4" name="__codelineno-13-4" href="#__codelineno-13-4"></a>- 模型蒸馏：模型蒸馏（Model Distillation）是指将一个小模型的输出作为大模型的输入，训练大模型，提升模型的性能。
<a id="__codelineno-13-5" name="__codelineno-13-5" href="#__codelineno-13-5"></a>- 数据增强：数据增强（Data Augmentation）是指通过对训练数据进行预处理，生成新的训练样本，提升模型的泛化能力。
<a id="__codelineno-13-6" name="__codelineno-13-6" href="#__codelineno-13-6"></a>- 模型架构：模型架构（Model Architecture）是指模型的结构，包括网络结构、层数、激活函数、参数数量等。
<a id="__codelineno-13-7" name="__codelineno-13-7" href="#__codelineno-13-7"></a>- 权重衰减：权重衰减（Weight Decay）是指模型训练时，更新模型参数时，对模型参数进行惩罚，以减少模型过拟合。
</code></pre></div></li>
</ul>
<h3 id="_11">加速训练<a class="headerlink" href="#_11" title="Permanent link">&para;</a></h3>
<ul>
<li>多卡训练：多卡训练（Multi-card Training）是指模型训练时，使用多个 GPU 进行并行训练。如DeepSpeed、DLRover 框架等提供了多卡训练的支持。</li>
<li>模型微调（Model Fine-tuning）是指在已有模型的基础上，微调模型参数，提升模型的性能。例如，微调预训练模型的参数，提升模型的性能。</li>
<li>裁剪梯度：裁剪梯度（Gradient Clipping）是指模型训练时，对梯度进行裁剪，以防止梯度爆炸。</li>
<li>延迟更新：延迟更新（Delayed Update）是指模型训练时，更新模型参数的频率稍微低一些，以减少模型更新的影响。</li>
<li>异步并行：异步并行（Asynchronous Parallel）是指模型训练时，使用异步方式进行并行训练，以提升训练速度。</li>
<li>模型并行（Model Parallelism）是指将模型的不同部分分布到不同的 GPU 上，并行计算，提升模型的训练速度。包括切分并行、切分并行、网格并行、流水线并行、张量并行等。</li>
<li>数据并行（Data Parallelism）是指将数据分布到不同的 GPU 上，并行计算，提升模型的训练速度。</li>
<li>混合精度训练（Mixed Precision Training，简称 Mixed Precision，简写 MP）是指在训练过程中同时使用 FP16 和 FP32 两种数据类型，通过混合精度训练可以加速模型的训练，同时减少内存占用，提升模型的精度。</li>
<li>半精度训练（Half Precision Training，简称 FP16，简写 FP16）是指在训练过程中使用 FP16 数据类型，通过半精度训练可以加速模型的训练，同时减少内存占用，提升模型的精度。</li>
<li>延迟加载（Lazy Loading）是指模型训练时，仅加载部分数据到内存，以节省内存。</li>
<li>内存优化：内存优化（Memory Optimization）是指模型训练时，减少模型的内存占用，提升模型的训练速度。</li>
<li>其他技巧：
<div class="highlight"><pre><span></span><code><a id="__codelineno-14-1" name="__codelineno-14-1" href="#__codelineno-14-1"></a>- 优化器（Optimizer）是指模型训练的算法，用于更新模型的参数。
<a id="__codelineno-14-2" name="__codelineno-14-2" href="#__codelineno-14-2"></a>- 数据增强（Data Augmentation）是指通过对训练数据进行预处理，生成新的训练样本，提升模型的泛化能力。
<a id="__codelineno-14-3" name="__codelineno-14-3" href="#__codelineno-14-3"></a>- 模型架构（Model Architecture）是指模型的结构，包括网络结构、层数、激活函数、参数数量等。
<a id="__codelineno-14-4" name="__codelineno-14-4" href="#__codelineno-14-4"></a>- 模型初始化（Model Initialization）是指模型参数的初始值，用于控制模型的训练起点。
<a id="__codelineno-14-5" name="__codelineno-14-5" href="#__codelineno-14-5"></a>- 模型蒸馏（Model Distillation）是指将一个小模型的输出作为大模型的输入，训练大模型，提升模型的性能。
<a id="__codelineno-14-6" name="__codelineno-14-6" href="#__codelineno-14-6"></a>- XLA（Accelerated Linear Algebra）是谷歌推出的一种编译器，可以加速线性代数运算，提升模型的训练速度。
<a id="__codelineno-14-7" name="__codelineno-14-7" href="#__codelineno-14-7"></a>- AMP（Automatic Mixed Precision）是一种技术，可以自动将 FP32 计算的部分转换为 FP16 计算，以节省内存和提升模型的训练速度。
</code></pre></div></li>
</ul>
<h3 id="_12">分布式训练方法<a class="headerlink" href="#_12" title="Permanent link">&para;</a></h3>
<p>如今的大模型训练，离不开各种分布式的训练框架，一般来说，并行策略包含：数据并行、模型并行、流水线并行。</p>
<p>流水线性并行和张量并行都是对模型本身进行划分，目的是利用有限的单卡显存训练更大的模型。简单来说，流水线并行水平划分模型，即按照层对模型进行划分；张量并行则是垂直划分模型。3D并行则是将流行线并行、张量并行和数据并行同时应用到模型训练中。</p>
<h4 id="_13">数据并行<a class="headerlink" href="#_13" title="Permanent link">&para;</a></h4>
<p>数据并行（Data Parallelism）是指将数据分布到不同的 GPU 上，并行计算，提升模型的训练速度。数据并行分为了两种模式：Data Parallel（DP）和 Distributed Data Parallel（DDP）。ZeRO-DP是一种显存高效的数据并行策略。</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-15-1" name="__codelineno-15-1" href="#__codelineno-15-1"></a>Data Parallel（DP），DP是一种单进程多线程的并行策略，Pytorch最早提供的一种数据并行方式，它基于单进程多线程进行实现的，它使用一个进程来计算模型权重，在每个批处理期间将数据分发到每个GPU。只能在单机上进行训练，步骤如下：
<a id="__codelineno-15-2" name="__codelineno-15-2" href="#__codelineno-15-2"></a>1、单进程控制多GPU，即本质上是单进程多线程；
<a id="__codelineno-15-3" name="__codelineno-15-3" href="#__codelineno-15-3"></a>2、首先将模型加载到主 GPU 上，再复制到各个指定从 GPU；
<a id="__codelineno-15-4" name="__codelineno-15-4" href="#__codelineno-15-4"></a>3、将输入数据按照 Batch 维度进行拆分，将数据切分成多个小批次，每个小批次在各个 GPU 独立进行 forward 计算；
<a id="__codelineno-15-5" name="__codelineno-15-5" href="#__codelineno-15-5"></a>4、每个进程分别计算小批次的梯度，并将梯度累加到一起；将结果同步给主 GPU 完成梯度计算和参数更新，将更新后的参数复制到各个 GPU。
<a id="__codelineno-15-6" name="__codelineno-15-6" href="#__codelineno-15-6"></a>5、由于其是单进程控制多个GPU，故会存在GPU之间负载不均衡的问题，主GPU负载较大。
<a id="__codelineno-15-7" name="__codelineno-15-7" href="#__codelineno-15-7"></a>
<a id="__codelineno-15-8" name="__codelineno-15-8" href="#__codelineno-15-8"></a>Distributed Data Parallel（DDP）：基于多进程进行实现的，每个进程都有独立的优化器，执行自己的更新过程。每个进程都执行相同的任务，并且每个进程都与所有其它进程通信，进程之间只传递梯度，这样网络通信就不再是瓶颈。DDP是一种多进程多线程的并行策略，可以跨多机进行训练，步骤如下：
<a id="__codelineno-15-9" name="__codelineno-15-9" href="#__codelineno-15-9"></a>1、多进程控制多GPU，即本质上是多进程多线程；DDP采用 AllReduce 架构，多进程的方式，突破锁的束缚。在单机和多机上都可以使用。
<a id="__codelineno-15-10" name="__codelineno-15-10" href="#__codelineno-15-10"></a>2、负载分散在每个 GPU 节点上，通信成本（时间）是恒定的，与 GPU 数量无关，等于V/B（参数量/带宽）。
<a id="__codelineno-15-11" name="__codelineno-15-11" href="#__codelineno-15-11"></a>3、DDP不需要通过主GPU分发全模型的参数到每个GPU上。
<a id="__codelineno-15-12" name="__codelineno-15-12" href="#__codelineno-15-12"></a>4、使用ring-all-reduce的方式进行通讯，随着 GPU 数量 N 增加，总传输量恒定。也就是理论上，随着GPU数量的增加，ring all-reduce有线性加速能力。
<a id="__codelineno-15-13" name="__codelineno-15-13" href="#__codelineno-15-13"></a>5、DDP可以实现多机多卡训练，只需要在不同机器上启动多个进程，每个进程指定不同的GPU，并设置环境变量。
<a id="__codelineno-15-14" name="__codelineno-15-14" href="#__codelineno-15-14"></a>
<a id="__codelineno-15-15" name="__codelineno-15-15" href="#__codelineno-15-15"></a>DP 和 DDP 的主要差异有以下几点：
<a id="__codelineno-15-16" name="__codelineno-15-16" href="#__codelineno-15-16"></a>1、DP 是基于单进程多线程的实现，只用于单机情况，而 DDP 是多进程实现的，每个 GPU 对应一个进程，适用于单机和多机情况，真正实现了分布式训练。
<a id="__codelineno-15-17" name="__codelineno-15-17" href="#__codelineno-15-17"></a>2、参数更新的方式不同，DDP在各进程梯度计算完成之后，各进程需要将梯度进行汇总平均，然后再由 rank=0 的进程，将其广播到所有进程后，各进程用该梯度来独立的更新参数（而 DP是梯度汇总到 GPU0，反向传播更新参数，再广播参数给其他剩余的 GPU）。
<a id="__codelineno-15-18" name="__codelineno-15-18" href="#__codelineno-15-18"></a>3、DP 存在 GPU 之间负载不均衡的问题，主 GPU 负载较大。DDP 采用 AllReduce 架构，多进程的方式，突破锁的束缚。
<a id="__codelineno-15-19" name="__codelineno-15-19" href="#__codelineno-15-19"></a>4、相较于DP，DDP负载和通信开销较小，训练更高效。DP 和 DDP 还有个缺点是至少有一块显卡保存完整的参数和梯度
<a id="__codelineno-15-20" name="__codelineno-15-20" href="#__codelineno-15-20"></a>
<a id="__codelineno-15-21" name="__codelineno-15-21" href="#__codelineno-15-21"></a>DDP 架构：
<a id="__codelineno-15-22" name="__codelineno-15-22" href="#__codelineno-15-22"></a>1、每个进程对应一个 GPU，进程之间通过网络通信；
<a id="__codelineno-15-23" name="__codelineno-15-23" href="#__codelineno-15-23"></a>2、每个进程只负责计算和梯度更新，不参与参数的更新；
<a id="__codelineno-15-24" name="__codelineno-15-24" href="#__codelineno-15-24"></a>3、每个进程都有完整的模型参数，可以并行计算梯度；
</code></pre></div>
<h4 id="_14">张量并行<a class="headerlink" href="#_14" title="Permanent link">&para;</a></h4>
<p>张量并行（Tensor Parallelism 简称 TP）是指张量操作划分到多个设备上，以加速计算或增加模型大小；对模型每一层的层内参数进行切分，即对参数矩阵切片，并将不同切片放到不同GPU上；将原本在单卡中的矩阵乘法，切分到不同卡中进行矩阵乘法。训练过程中，正向和反向传播计算出的数据通过使用 All gather 或者 All reduce 的方法完成整合。张量并行的核心就是将矩阵乘法进行拆分（行并行、列并行），从而降低模型对单卡的显存需求，适用于模型单层网络参数较大的情况。</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-16-1" name="__codelineno-16-1" href="#__codelineno-16-1"></a>步骤：
<a id="__codelineno-16-2" name="__codelineno-16-2" href="#__codelineno-16-2"></a>1、将模型切分成多个子网络，每个子网络在不同的GPU上运行；
<a id="__codelineno-16-3" name="__codelineno-16-3" href="#__codelineno-16-3"></a>2、每个子网络的输入和输出张量被切分成多个子张量，并在不同GPU上并行计算；
<a id="__codelineno-16-4" name="__codelineno-16-4" href="#__codelineno-16-4"></a>3、每个子张量的梯度被累加到一起，并在主GPU上完成参数更新。
<a id="__codelineno-16-5" name="__codelineno-16-5" href="#__codelineno-16-5"></a>
<a id="__codelineno-16-6" name="__codelineno-16-6" href="#__codelineno-16-6"></a>优点：
<a id="__codelineno-16-7" name="__codelineno-16-7" href="#__codelineno-16-7"></a>1、张量并行可以有效地利用多卡的计算资源，提升模型的训练速度；
<a id="__codelineno-16-8" name="__codelineno-16-8" href="#__codelineno-16-8"></a>2、张量并行可以有效地减少模型的内存占用，提升模型的训练效率；
<a id="__codelineno-16-9" name="__codelineno-16-9" href="#__codelineno-16-9"></a>3、张量并行可以有效地提升模型的并行度，进一步提升模型的训练速度。
<a id="__codelineno-16-10" name="__codelineno-16-10" href="#__codelineno-16-10"></a>
<a id="__codelineno-16-11" name="__codelineno-16-11" href="#__codelineno-16-11"></a>transformer为例：该策略会把 Masked Multi Self Attention 和 Feed Forward 都进行切分以并行化。利用 Transformers 网络的结构，通过添加一些同步原语来创建一个简单的模型并行实现。Transformer中的主要部件是全连接层和注意力机制，其核心都是矩阵乘法。
<a id="__codelineno-16-12" name="__codelineno-16-12" href="#__codelineno-16-12"></a>
<a id="__codelineno-16-13" name="__codelineno-16-13" href="#__codelineno-16-13"></a>缺点：
<a id="__codelineno-16-14" name="__codelineno-16-14" href="#__codelineno-16-14"></a>1、张量并行的实现难度较大，需要对模型架构进行改造，增加额外的计算和通信开销；
<a id="__codelineno-16-15" name="__codelineno-16-15" href="#__codelineno-16-15"></a>2、张量并行的实现依赖于硬件的支持，目前主流的张量并行硬件有 NVIDIA A100 和 NVIDIA A30 系列；
<a id="__codelineno-16-16" name="__codelineno-16-16" href="#__codelineno-16-16"></a>3、张量并行的实现依赖于特定的优化算法，目前主流的优化算法有 Adam、AdaGrad、AdaMax、AdamW、Nadam、RMSprop、SGD 等。
<a id="__codelineno-16-17" name="__codelineno-16-17" href="#__codelineno-16-17"></a>4、张量并行的实现需要对模型架构进行改造，增加额外的计算和通信开销，这会影响模型的可移植性，降低模型的可解释性，降低模型的可扩展性，增加模型的训练难度，因此，张量并行目前还处于实验阶段，并不是所有模型都适合张量并行。
<a id="__codelineno-16-18" name="__codelineno-16-18" href="#__codelineno-16-18"></a>5、若环境是多机多卡，张量并行所需的all-reduce通信需要跨服务器进行链接，这比单机多GPU服务器内的高带宽通信要慢；
<a id="__codelineno-16-19" name="__codelineno-16-19" href="#__codelineno-16-19"></a>6、高度的模型并行会产生很多小矩阵乘法，这可能会降低GPU的利用率。
<a id="__codelineno-16-20" name="__codelineno-16-20" href="#__codelineno-16-20"></a>7、需要保证输入一致，不能使用数据并行
</code></pre></div>
<h4 id="_15">流水线并行<a class="headerlink" href="#_15" title="Permanent link">&para;</a></h4>
<p>流水线并行（Pipeline Parallelism 简称PP）目标是训练更大的模型，将不同的层（layer）分配给指定 GPU 进行计算，流水线并行只需其之间点对点地通讯传递部分激活 (activations)缓存，而不需要全局同步。</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-17-1" name="__codelineno-17-1" href="#__codelineno-17-1"></a>具体步骤包括：
<a id="__codelineno-17-2" name="__codelineno-17-2" href="#__codelineno-17-2"></a>1、在流水线并行之中，一个模型的各层会在多个GPU上做切分。
<a id="__codelineno-17-3" name="__codelineno-17-3" href="#__codelineno-17-3"></a>2、一个批次（batch）被分割成较小的微批（microbatches），并在这些微批上进行流水线式执行。
<a id="__codelineno-17-4" name="__codelineno-17-4" href="#__codelineno-17-4"></a>3、在每个GPU上，流水线式执行各层的前向传播和反向传播。通过流水线并行，一个模型的层被分散到多个设备上。
<a id="__codelineno-17-5" name="__codelineno-17-5" href="#__codelineno-17-5"></a>4、当用于具有相同transformer块重复的模型时，每个设备可以被分配相同数量的transformer层。
<a id="__codelineno-17-6" name="__codelineno-17-6" href="#__codelineno-17-6"></a>5、在流水线模型并行中，训练会在一个设备上执行一组操作，然后将输出传递到流水线中下一个设备，下一个设备将执行另一组不同操作。
</code></pre></div>
<p>流水线并行的方法解决了超大模型无法在单设备上装下的难题，也解决了机器之间的通信开销的问题，使得每台机器的数据传输量跟总的网络大小、机器总数、并行规模无关。</p>
<ul>
<li>常用的流水线方法有G-pipe、PipeDream等。</li>
</ul>
<div class="highlight"><pre><span></span><code><a id="__codelineno-18-1" name="__codelineno-18-1" href="#__codelineno-18-1"></a>朴素层：
<a id="__codelineno-18-2" name="__codelineno-18-2" href="#__codelineno-18-2"></a>  1、并行的过程：在某一时刻仅有1个GPU工作。并且每个timesep花费的时间也比较长，因为GPU需要跑完整个minibatch的前向传播。
<a id="__codelineno-18-3" name="__codelineno-18-3" href="#__codelineno-18-3"></a>  2、缺点：
<a id="__codelineno-18-4" name="__codelineno-18-4" href="#__codelineno-18-4"></a>     - 低GPU利用率，在任意时刻，有且仅有一个GPU在工作，其他GPU都是空闲的。
<a id="__codelineno-18-5" name="__codelineno-18-5" href="#__codelineno-18-5"></a>     - 计算和通信没有重叠。在发送前向传播的中间结果(FWD)或者反向传播的中间结果(BWD)时，GPU也是空闲的。
<a id="__codelineno-18-6" name="__codelineno-18-6" href="#__codelineno-18-6"></a>     - 高显存占用，在训练过程中，GPU1需要保存整个小批量（minibatch）的所有激活，直至最后完成参数更新。如果batch size很大，这将对显存带来巨大的挑战。
<a id="__codelineno-18-7" name="__codelineno-18-7" href="#__codelineno-18-7"></a>
<a id="__codelineno-18-8" name="__codelineno-18-8" href="#__codelineno-18-8"></a>GPipe：
<a id="__codelineno-18-9" name="__codelineno-18-9" href="#__codelineno-18-9"></a>   1、GPipe通过将minibatch划分为更小且相等尺寸的microbatch来提高效率。具体来说，让每个microbatch独立的计算前后向传播，然后将每个mircobatch的梯度相加，就能得到整个batch的梯度。由于每个层仅在一个GPU上，对mircobatch的梯度求和仅需要在本地进行即可，不需要通信。
<a id="__codelineno-18-10" name="__codelineno-18-10" href="#__codelineno-18-10"></a>   2、GPipe的调度中，每个timestep上花费的时间要比朴素层并行更短，因为每个GPU仅需要处理microbatch。每个GPU可以同时处理多个microbatch，因此可以提高GPU的利用率。
<a id="__codelineno-18-11" name="__codelineno-18-11" href="#__codelineno-18-11"></a>   3、GPipe的Bubbles问题，bubbles指的是流水线中没有进行任何有效工作的点。这是由于操作之间的依赖导致的。bubbles浪费时间的比例依赖于pipeline的深度 和mincrobatch的数量。因此，增大microbatch的数量m，可以降低bubbles的比例。
<a id="__codelineno-18-12" name="__codelineno-18-12" href="#__codelineno-18-12"></a>   4、GPipe的并行度较高，但通信开销较大。训练过程中，每个timestep上的计算时间比上一次要长，因为每个timestep上的计算时间都包含了一个microbatch的计算时间，以及一个microbatch的梯度求和时间。
<a id="__codelineno-18-13" name="__codelineno-18-13" href="#__codelineno-18-13"></a>   6、GPipe的通信开销较低，因为每个GPU仅需要发送和接收部分激活，而不是整个minibatch。
<a id="__codelineno-18-14" name="__codelineno-18-14" href="#__codelineno-18-14"></a>   7、GPipe的内存占用较低，因为每个GPU仅需要保存部分激活，而不是整个minibatch。
<a id="__codelineno-18-15" name="__codelineno-18-15" href="#__codelineno-18-15"></a>   8、GPipe的显存需求，增大batch size就会线性增大需要被缓存激活的显存需求。GPU需要在前向传播至反向传播这段时间内缓存激活(activations)。为了解决显存的问题，使用了gradient checkpointing。该技术不需要缓存所有的激活，而是在反向传播的过程中重新计算激活。这降低了对显存的需求，但是增加了计算代价。
<a id="__codelineno-18-16" name="__codelineno-18-16" href="#__codelineno-18-16"></a>
<a id="__codelineno-18-17" name="__codelineno-18-17" href="#__codelineno-18-17"></a>PipeDream：
<a id="__codelineno-18-18" name="__codelineno-18-18" href="#__codelineno-18-18"></a>   1、GPipe需要等所有的microbatch前向传播完成后，才会开始反向传播。PipeDream则是当一个microbatch的前向传播完成后，立即进入反向传播阶段。 理论上，反向传播完成后就可以丢弃掉对应microbatch缓存的激活。由于PipeDream的反向传播完成的要比GPipe早，因此也会减少显存的需求。
<a id="__codelineno-18-19" name="__codelineno-18-19" href="#__codelineno-18-19"></a>   2、PipeDream在bubbles上与GPipe没有区别，但是由于PipeDream释放显存的时间更早，因此会降低对显存的需求。
<a id="__codelineno-18-20" name="__codelineno-18-20" href="#__codelineno-18-20"></a>
<a id="__codelineno-18-21" name="__codelineno-18-21" href="#__codelineno-18-21"></a>合并数据并行和流水线并行：
<a id="__codelineno-18-22" name="__codelineno-18-22" href="#__codelineno-18-22"></a>   1、数据并行和流水线并行是正交的，可以同时使用。
<a id="__codelineno-18-23" name="__codelineno-18-23" href="#__codelineno-18-23"></a>   2、对于流水线并行：每个GPU需要与下个流水线阶段(前向传播)或者上个流水线阶段(反向传播)进行通信。
<a id="__codelineno-18-24" name="__codelineno-18-24" href="#__codelineno-18-24"></a>   3、对于数据并行：每个GPU需要与分配了相同层的GPU进行通信。所有层的副本需要AllReduce对梯度进行平均。
<a id="__codelineno-18-25" name="__codelineno-18-25" href="#__codelineno-18-25"></a>   4、将所有GPU上形成子组，并在子组中使用集合通信。任意给定的GPU都会有两部分的通信，一个是包含所有相同层的GPU(数据并行)，另一个与不同层的GPU(流水线并行)。
<a id="__codelineno-18-26" name="__codelineno-18-26" href="#__codelineno-18-26"></a>   5、子组间的通信需要使用集合通信，比如AllReduce。
<a id="__codelineno-18-27" name="__codelineno-18-27" href="#__codelineno-18-27"></a>   6、子组内的通信可以使用点对点通信。比如每个GPU只需要与其子组内的其他GPU通信。
<a id="__codelineno-18-28" name="__codelineno-18-28" href="#__codelineno-18-28"></a>   7、子组内的通信需要使用异步通信，以提高训练速度。比如每个GPU可以同时发送和接收消息。
<a id="__codelineno-18-29" name="__codelineno-18-29" href="#__codelineno-18-29"></a>   8、子组间的通信需要使用同步通信，以保证模型的一致性。比如每个子组的梯度需要AllReduce之后，才能更新模型参数。
</code></pre></div>
<h4 id="3d">3D 并行<a class="headerlink" href="#3d" title="Permanent link">&para;</a></h4>
<p>3D 并行（3D Parallelism）是指将数据并行(DP)、张量并行(TP)和流水线并行(PP)同时应用到模型训练中。3D 并行的核心是将模型切分成多个子网络，每个子网络在不同的 GPU 上运行，并行计算。
<div class="highlight"><pre><span></span><code><a id="__codelineno-19-1" name="__codelineno-19-1" href="#__codelineno-19-1"></a>3D并行分析：
<a id="__codelineno-19-2" name="__codelineno-19-2" href="#__codelineno-19-2"></a>  1、3D并行的计算量是 DP、TP、PP 的组合，因此，3D并行的计算量是 DP、TP、PP 的乘积。
<a id="__codelineno-19-3" name="__codelineno-19-3" href="#__codelineno-19-3"></a>  2、模型并行是三种策略中通信开销最大的，所以优先将模型并行组放置在一个节点中，以利用较大的节点内带宽。
<a id="__codelineno-19-4" name="__codelineno-19-4" href="#__codelineno-19-4"></a>  3、流水线并行通信量最低，因此在不同节点之间调度流水线，这将不受通信带宽的限制。
<a id="__codelineno-19-5" name="__codelineno-19-5" href="#__codelineno-19-5"></a>  4、若张量并行没有跨节点，则数据并行也不需要跨节点；否则数据并行组也需要跨节点。
<a id="__codelineno-19-6" name="__codelineno-19-6" href="#__codelineno-19-6"></a>
<a id="__codelineno-19-7" name="__codelineno-19-7" href="#__codelineno-19-7"></a>分析总结：流水线并行和张量并行减少了单个显卡的显存消耗，提高了显存效率。但是，模型划分的太多会增加通信开销，从而降低计算效率。ZeRO-DP不仅能够通过将优化器状态进行划分来改善显存效率，而且还不会显著的增加通信开销。
</code></pre></div></p>
<h4 id="_16">其他分布式训练方法<a class="headerlink" href="#_16" title="Permanent link">&para;</a></h4>
<ul>
<li>半同步并行（Half-Synchronous Parallel）：半同步并行（Half-Synchronous Parallel，简称 HSP）是指模型训练时，使用半同步的并行策略，即模型训练时，各个 GPU 之间同步参数的更新，但各个 GPU 之间不一定同步梯度的更新。</li>
<li>异步并行（Asynchronous Parallel）：异步并行（Asynchronous Parallel，简称 AP）是指模型训练时，使用异步的并行策略，即模型训练时，各个 GPU 之间不一定同步参数的更新，但各个 GPU 之间同步梯度的更新。</li>
<li>聚合通信（Aggregation Communication）：聚合通信（Aggregation Communication，简称 AC）是指模型训练时，使用聚合通信的策略，即模型训练时，各个 GPU 之间同步参数的更新，并使用聚合通信的方式同步梯度的更新。</li>
<li>其他分布式训练方法：其他分布式训练方法（Other Distributed Training Method）是指模型训练时，用于分布式训练的其他方法。</li>
</ul>
<h3 id="_17">训练框架<a class="headerlink" href="#_17" title="Permanent link">&para;</a></h3>
<p>训练框架（Training Framework）是指模型训练的框架，包括 PyTorch、TensorFlow、DeepSpeed、DLRover 等常用框架。</p>
<h4 id="tensorflow">TensorFlow<a class="headerlink" href="#tensorflow" title="Permanent link">&para;</a></h4>
<p>TensorFlow 是 Google 推出的深度学习框架，是目前最流行的深度学习框架。它被用来构建各种类型的机器学习模型，例如图像识别、语音识别、自然语言处理等。</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-20-1" name="__codelineno-20-1" href="#__codelineno-20-1"></a>TensorFlow的特点：
<a id="__codelineno-20-2" name="__codelineno-20-2" href="#__codelineno-20-2"></a>- 开源：TensorFlow 是一个开源项目，其代码可以免费获取。
<a id="__codelineno-20-3" name="__codelineno-20-3" href="#__codelineno-20-3"></a>- 跨平台：TensorFlow 可以运行在 Linux、Windows、macOS 等多种操作系统上。
<a id="__codelineno-20-4" name="__codelineno-20-4" href="#__codelineno-20-4"></a>- 灵活：TensorFlow 是一个灵活的框架，可以构建各种类型的模型，包括卷积神经网络（CNN）、循环神经网络（RNN）、递归神经网络（RNN）、深度置信网络（DCN）等。
<a id="__codelineno-20-5" name="__codelineno-20-5" href="#__codelineno-20-5"></a>- 高性能：TensorFlow 具有高性能，可以运行在 GPU 和 TPU 上，并提供分布式训练的支持。
<a id="__codelineno-20-6" name="__codelineno-20-6" href="#__codelineno-20-6"></a>- 易用：TensorFlow 提供了易用的 API，可以快速构建模型。
<a id="__codelineno-20-7" name="__codelineno-20-7" href="#__codelineno-20-7"></a>
<a id="__codelineno-20-8" name="__codelineno-20-8" href="#__codelineno-20-8"></a>TensorFlow主要有以下几个基本概念：
<a id="__codelineno-20-9" name="__codelineno-20-9" href="#__codelineno-20-9"></a>- Tensor：TensorFlow中最基本的数据结构，是一个多维数组，可以表示向量、矩阵和高维数组等。
<a id="__codelineno-20-10" name="__codelineno-20-10" href="#__codelineno-20-10"></a>- Graph：TensorFlow计算图是一种数据流图，表示了计算中各个操作节点之间的数据依赖关系。
<a id="__codelineno-20-11" name="__codelineno-20-11" href="#__codelineno-20-11"></a>- Operation：Operation是TensorFlow中的一种操作，用于在计算图上执行各种操作，例如张量运算、赋值和变量初始化等。
<a id="__codelineno-20-12" name="__codelineno-20-12" href="#__codelineno-20-12"></a>- Session：Session是TensorFlow中的一种执行环境，用于在计算图上执行各种计算操作。
<a id="__codelineno-20-13" name="__codelineno-20-13" href="#__codelineno-20-13"></a>- Variable：Variable是TensorFlow中的一种特殊操作，用于存储模型参数，例如权重和偏置。
<a id="__codelineno-20-14" name="__codelineno-20-14" href="#__codelineno-20-14"></a>- Placeholder：Placeholder是TensorFlow中的一种占位符，用于在计算图上定义输入数据，等待实际数据输入。
<a id="__codelineno-20-15" name="__codelineno-20-15" href="#__codelineno-20-15"></a>- Feed：Feed是Session中的一种输入方式，用于向计算图提供输入数据。
<a id="__codelineno-20-16" name="__codelineno-20-16" href="#__codelineno-20-16"></a>- Saver：Saver是TensorFlow中的一种保存和加载模型参数的工具，用于保存和加载模型参数。
<a id="__codelineno-20-17" name="__codelineno-20-17" href="#__codelineno-20-17"></a>- TensorBoard：TensorBoard是TensorFlow中的一种可视化工具，用于可视化计算图、模型参数、训练过程等。
<a id="__codelineno-20-18" name="__codelineno-20-18" href="#__codelineno-20-18"></a>- Estimator：Estimator是TensorFlow中的一种高级API，用于简化模型训练和评估的过程。
<a id="__codelineno-20-19" name="__codelineno-20-19" href="#__codelineno-20-19"></a>- Dataset：Dataset是TensorFlow中的一种数据集，用于存储和管理数据。
<a id="__codelineno-20-20" name="__codelineno-20-20" href="#__codelineno-20-20"></a>
<a id="__codelineno-20-21" name="__codelineno-20-21" href="#__codelineno-20-21"></a>TensorFlow的使用场景非常广泛，以下是一些常见的场景：
<a id="__codelineno-20-22" name="__codelineno-20-22" href="#__codelineno-20-22"></a>- 图像识别：使用TensorFlow可以构建卷积神经网络（CNN）等模型，用于图像识别和分类等任务。
<a id="__codelineno-20-23" name="__codelineno-20-23" href="#__codelineno-20-23"></a>- 语音识别：使用TensorFlow可以构建循环神经网络（RNN）等模型，用于语音识别和语音合成等任务。
<a id="__codelineno-20-24" name="__codelineno-20-24" href="#__codelineno-20-24"></a>- 自然语言处理：使用TensorFlow可以构建深度学习模型，例如循环神经网络（RNN）和长短期记忆网络（LSTM），用于自然语言处理任务，例如文本生成和情感分析等。
<a id="__codelineno-20-25" name="__codelineno-20-25" href="#__codelineno-20-25"></a>- 强化学习：使用TensorFlow可以构建强化学习模型，例如深度Q网络（DQN）和策略梯度方法，用于实现自主学习和决策等任务。
<a id="__codelineno-20-26" name="__codelineno-20-26" href="#__codelineno-20-26"></a>- 推荐系统：使用TensorFlow可以构建推荐系统模型，例如矩阵分解机（MF）和因子分解机（FFM），用于推荐系统任务，例如个性化推荐等。
<a id="__codelineno-20-27" name="__codelineno-20-27" href="#__codelineno-20-27"></a>
<a id="__codelineno-20-28" name="__codelineno-20-28" href="#__codelineno-20-28"></a>TensorFlow的缺点：
<a id="__codelineno-20-29" name="__codelineno-20-29" href="#__codelineno-20-29"></a>- 学习曲线陡峭：TensorFlow 的学习曲线陡峭，需要花费大量时间和资源进行模型调参。
<a id="__codelineno-20-30" name="__codelineno-20-30" href="#__codelineno-20-30"></a>- 易用性差：TensorFlow 的易用性较低，需要编写大量代码，且代码可读性较差。
<a id="__codelineno-20-31" name="__codelineno-20-31" href="#__codelineno-20-31"></a>- 性能不足：TensorFlow 的性能较低，在某些任务上，其训练速度较慢。如：图像分类任务。
</code></pre></div>
<h4 id="pytorch">PyTorch<a class="headerlink" href="#pytorch" title="Permanent link">&para;</a></h4>
<p>PyTorch是基于Python的一种开源机器学习库，具有灵活性和直观性，特别适合于动态计算图。PyTorch是Facebook的一个项目，旨在提供一种简单易用的深度学习框架。PyTorch的主要应用场景是：深度学习，自然语言处理，计算机视觉，强化学习等领域。</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-21-1" name="__codelineno-21-1" href="#__codelineno-21-1"></a>PyTorch的特点：
<a id="__codelineno-21-2" name="__codelineno-21-2" href="#__codelineno-21-2"></a>- 开源：PyTorch 是一个开源项目，其代码可以免费获取。
<a id="__codelineno-21-3" name="__codelineno-21-3" href="#__codelineno-21-3"></a>- 跨平台：PyTorch 可以运行在 Linux、Windows、macOS 等多种操作系统上。
<a id="__codelineno-21-4" name="__codelineno-21-4" href="#__codelineno-21-4"></a>- 灵活：PyTorch 是一个灵活的框架，可以构建各种类型的模型，包括卷积神经网络（CNN）、循环神经网络（RNN）、递归神经网络（RNN）、深度置信网络（DCN）等。
<a id="__codelineno-21-5" name="__codelineno-21-5" href="#__codelineno-21-5"></a>- 高性能：PyTorch 具有高性能，可以运行在 GPU 和 TPU 上，并提供分布式训练的支持。
<a id="__codelineno-21-6" name="__codelineno-21-6" href="#__codelineno-21-6"></a>- 易用：PyTorch 提供了易用的 API，可以快速构建模型。
<a id="__codelineno-21-7" name="__codelineno-21-7" href="#__codelineno-21-7"></a>
<a id="__codelineno-21-8" name="__codelineno-21-8" href="#__codelineno-21-8"></a>PyTorch主要有以下几个基本概念：
<a id="__codelineno-21-9" name="__codelineno-21-9" href="#__codelineno-21-9"></a>- Tensor：PyTorch中的Tensor是一个多维数组（一个张量），用于存储和操作数据，类似于Numpy中的ndarray。
<a id="__codelineno-21-10" name="__codelineno-21-10" href="#__codelineno-21-10"></a>- Module：Module是PyTorch中的一种容器，用于封装神经网络层、损失函数等。
<a id="__codelineno-21-11" name="__codelineno-21-11" href="#__codelineno-21-11"></a>- Optimizer：Optimizer是PyTorch中的一种优化器，用于更新模型参数。
<a id="__codelineno-21-12" name="__codelineno-21-12" href="#__codelineno-21-12"></a>- DataLoader：DataLoader是PyTorch中的一种数据加载器，用于加载和预处理数据。
<a id="__codelineno-21-13" name="__codelineno-21-13" href="#__codelineno-21-13"></a>- Dataset：Dataset是PyTorch中的一种数据集，用于存储和管理数据。
<a id="__codelineno-21-14" name="__codelineno-21-14" href="#__codelineno-21-14"></a>- Loss Function：Loss Function是PyTorch中的一种损失函数，用于衡量模型的预测值和真实值之间的差距。
<a id="__codelineno-21-15" name="__codelineno-21-15" href="#__codelineno-21-15"></a>- Metric：Metric是PyTorch中的一种指标，用于评估模型的性能。
<a id="__codelineno-21-16" name="__codelineno-21-16" href="#__codelineno-21-16"></a>- Device：Device是PyTorch中的一种设备，用于指定模型运行的设备。
<a id="__codelineno-21-17" name="__codelineno-21-17" href="#__codelineno-21-17"></a>- Gradient：Gradient是PyTorch中的一种梯度，用于计算模型参数的梯度。
<a id="__codelineno-21-18" name="__codelineno-21-18" href="#__codelineno-21-18"></a>- Autograd：Autograd是PyTorch中的一种自动求导引擎，用于自动计算梯度。
<a id="__codelineno-21-19" name="__codelineno-21-19" href="#__codelineno-21-19"></a>- DistributedDataParallel：DistributedDataParallel是PyTorch中的一种分布式数据并行，用于在多台机器上并行训练模型。
<a id="__codelineno-21-20" name="__codelineno-21-20" href="#__codelineno-21-20"></a>- DistributedSampler：DistributedSampler是PyTorch中的一种分布式采样器，用于在多台机器上并行训练模型。
<a id="__codelineno-21-21" name="__codelineno-21-21" href="#__codelineno-21-21"></a>- DistributedTrainer：DistributedTrainer是PyTorch中的一种分布式训练器，用于在多台机器上并行训练模型。
<a id="__codelineno-21-22" name="__codelineno-21-22" href="#__codelineno-21-22"></a>- DistributedModel：DistributedModel是PyTorch中的一种分布式模型，用于在多台机器上并行训练模型。
<a id="__codelineno-21-23" name="__codelineno-21-23" href="#__codelineno-21-23"></a>- DistributedOptimizer：DistributedOptimizer是PyTorch中的一种分布式优化器，用于在多台机器上并行训练模型。
<a id="__codelineno-21-24" name="__codelineno-21-24" href="#__codelineno-21-24"></a>- DistributedLoss：DistributedLoss是PyTorch中的一种分布式损失函数，用于在多台机器上并行训练模型。
<a id="__codelineno-21-25" name="__codelineno-21-25" href="#__codelineno-21-25"></a>- DistributedMetric：DistributedMetric是PyTorch中的一种分布式指标，用于在多台机器上并行训练模型。
<a id="__codelineno-21-26" name="__codelineno-21-26" href="#__codelineno-21-26"></a>- DistributedSampler：DistributedSampler是PyTorch中的一种分布式采样器，用于在多台机器上并行训练模型。
<a id="__codelineno-21-27" name="__codelineno-21-27" href="#__codelineno-21-27"></a>- DistributedDataParallel：DistributedDataParallel是PyTorch中的一种分布式数据并行，用于在多台机器上并行训练模型。
<a id="__codelineno-21-28" name="__codelineno-21-28" href="#__codelineno-21-28"></a>
<a id="__codelineno-21-29" name="__codelineno-21-29" href="#__codelineno-21-29"></a>与TensorFlow相比，PyTorch的的优点在于：
<a id="__codelineno-21-30" name="__codelineno-21-30" href="#__codelineno-21-30"></a>- 灵活性： PyTorch使用动态图（Dynamic Graph）是指在运行时，根据输入数据的大小，自动调整计算图的结构，以节省内存。使得代码更加简洁易懂，更加灵活，适合于小规模数据和尝试实验。
<a id="__codelineno-21-31" name="__codelineno-21-31" href="#__codelineno-21-31"></a>- 速度：PyTorch的计算速度更快，在相同的硬件上，PyTorch的训练速度通常比TensorFlow快。
<a id="__codelineno-21-32" name="__codelineno-21-32" href="#__codelineno-21-32"></a>- 易用性：PyTorch的接口和文档更加简单易懂，调试代码更加方便，并且有许多社区贡献的资源和工具。
<a id="__codelineno-21-33" name="__codelineno-21-33" href="#__codelineno-21-33"></a>- 可视化：PyTorch通过TensorBoard和Visdom等可视化工具，可视化神经网络训练过程中的结果，方便数据分析。
<a id="__codelineno-21-34" name="__codelineno-21-34" href="#__codelineno-21-34"></a>- 社区支持：PyTorch有大量的社区贡献的资源和工具，可以帮助开发者解决常见问题。
<a id="__codelineno-21-35" name="__codelineno-21-35" href="#__codelineno-21-35"></a>
<a id="__codelineno-21-36" name="__codelineno-21-36" href="#__codelineno-21-36"></a>PyTorch的缺点：
<a id="__codelineno-21-37" name="__codelineno-21-37" href="#__codelineno-21-37"></a>- 易用性和灵活性带来的缺点是，PyTorch在大型数据集上需要额外的工作来优化它的计算效率，而且同时也影响了代码的可维护性。
<a id="__codelineno-21-38" name="__codelineno-21-38" href="#__codelineno-21-38"></a>- PyTorch缺乏安全性，因此有时可能会面临被恶意代码攻击的风险。
<a id="__codelineno-21-39" name="__codelineno-21-39" href="#__codelineno-21-39"></a>- 缺乏专业的深度学习库，导致其功能有限。
<a id="__codelineno-21-40" name="__codelineno-21-40" href="#__codelineno-21-40"></a>
<a id="__codelineno-21-41" name="__codelineno-21-41" href="#__codelineno-21-41"></a>总的来说，PyTorch是一个优秀的深度学习框架，适用于小规模数据和尝试实验，但在大型数据集上需要额外的工作来优化它的计算效率。
</code></pre></div>
<h4 id="deepspeed">DeepSpeed<a class="headerlink" href="#deepspeed" title="Permanent link">&para;</a></h4>
<p>DeepSpeed 是由微软推出的开源分布式工具，旨在提高大规模模型训练的效率和可扩展性。并提供分布式训练的支持其集合了分布式训练、推断、压缩等高效模块。它通过多种技术手段来加速训练，包括自动混合精度训练、自动并行、自动切分、自动混合精度优化器、本地模式混合精度等。
DeepSpeed还提供了一些辅助工具，如分布式训练管理、内存优化和模型压缩等，以帮助开发者更好地管理和优化大规模深度学习训练任务。</p>
<p>此外，deepspeed基于pytorch构建，只需要简单修改即可迁移。 DeepSpeed已经在许多大规模深度学习项目中得到了应用，包括语言模型、图像分类、目标检测等</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-22-1" name="__codelineno-22-1" href="#__codelineno-22-1"></a>DeepSpeed的特点：
<a id="__codelineno-22-2" name="__codelineno-22-2" href="#__codelineno-22-2"></a>- 开源：DeepSpeed 是一个开源项目，其代码可以免费获取。
<a id="__codelineno-22-3" name="__codelineno-22-3" href="#__codelineno-22-3"></a>- 跨平台：DeepSpeed 可以运行在 Linux、Windows、macOS 等多种操作系统上。
<a id="__codelineno-22-4" name="__codelineno-22-4" href="#__codelineno-22-4"></a>- 灵活：DeepSpeed 是一个灵活的框架，可以构建各种类型的模型，包括卷积神经网络（CNN）、循环神经网络（RNN）、递归神经网络（RNN）、深度置信网络（DCN）等。
<a id="__codelineno-22-5" name="__codelineno-22-5" href="#__codelineno-22-5"></a>- 高性能：DeepSpeed 具有高性能，可以运行在 GPU 和 TPU 上，并提供分布式训练的支持。
<a id="__codelineno-22-6" name="__codelineno-22-6" href="#__codelineno-22-6"></a>- 易用：DeepSpeed 提供了易用的 API，可以快速构建模型。
<a id="__codelineno-22-7" name="__codelineno-22-7" href="#__codelineno-22-7"></a>
<a id="__codelineno-22-8" name="__codelineno-22-8" href="#__codelineno-22-8"></a>DeepSpeed主要有以下几个基本概念：
<a id="__codelineno-22-9" name="__codelineno-22-9" href="#__codelineno-22-9"></a>- ZeRO：ZeRO 是 DeepSpeed 中的一种优化器，可以实现模型并行和数据并行。
<a id="__codelineno-22-10" name="__codelineno-22-10" href="#__codelineno-22-10"></a>- Pipeline Parallelism：Pipeline Parallelism 是 DeepSpeed 中的一种并行策略，可以实现模型并行。
<a id="__codelineno-22-11" name="__codelineno-22-11" href="#__codelineno-22-11"></a>- Offload：Offload 是 DeepSpeed 中的一种技术，可以实现模型的卸载。
<a id="__codelineno-22-12" name="__codelineno-22-12" href="#__codelineno-22-12"></a>- ZeRO-Offload：ZeRO-Offload 是 DeepSpeed 中的一种优化器，可以实现模型并行、数据并行和模型的卸载。
<a id="__codelineno-22-13" name="__codelineno-22-13" href="#__codelineno-22-13"></a>- ZeRO-3：ZeRO-3 是 DeepSpeed 中的一种优化器，可以实现模型并行、数据并行和模型的卸载。
<a id="__codelineno-22-14" name="__codelineno-22-14" href="#__codelineno-22-14"></a>    ZeRO-3的原理：
<a id="__codelineno-22-15" name="__codelineno-22-15" href="#__codelineno-22-15"></a>      1、对优化器Optimizer、gradients、model parameter进行分片
<a id="__codelineno-22-16" name="__codelineno-22-16" href="#__codelineno-22-16"></a>         a. AllReduce操作可以被拆分为Reduce与allgather操作的结合。
<a id="__codelineno-22-17" name="__codelineno-22-17" href="#__codelineno-22-17"></a>         b. 模型的每一层拥有该层的完整参数，并且整个层能够直接被一个GPU装下。所以计算前向的时候，除了当前rank需要的层之外，其余的层的参数可以抛弃。
<a id="__codelineno-22-18" name="__codelineno-22-18" href="#__codelineno-22-18"></a>      2、每个rank计算forward过程
<a id="__codelineno-22-19" name="__codelineno-22-19" href="#__codelineno-22-19"></a>         a. 使用AllGather获取模型该层所需的前置的层的参数。
<a id="__codelineno-22-20" name="__codelineno-22-20" href="#__codelineno-22-20"></a>         b. 结束后释放掉不属于该rank分片的层的参数。
<a id="__codelineno-22-21" name="__codelineno-22-21" href="#__codelineno-22-21"></a>      3、每个rank计算backward过程
<a id="__codelineno-22-22" name="__codelineno-22-22" href="#__codelineno-22-22"></a>         a. 使用AllGather获取该层所需要的层之前过程的参数。
<a id="__codelineno-22-23" name="__codelineno-22-23" href="#__codelineno-22-23"></a>         b. 结束后释放掉不属于该rank分片的层的参数。
<a id="__codelineno-22-24" name="__codelineno-22-24" href="#__codelineno-22-24"></a>      4、使用Reduce对当前分片的参数的梯度进行累加。
<a id="__codelineno-22-25" name="__codelineno-22-25" href="#__codelineno-22-25"></a>      5、使用AllReduce对梯度进行同步,并更新模型参数。让每个rank根据聚合的梯度，独立更新参数。
<a id="__codelineno-22-26" name="__codelineno-22-26" href="#__codelineno-22-26"></a>    ZeRO-3的优点:
<a id="__codelineno-22-27" name="__codelineno-22-27" href="#__codelineno-22-27"></a>      1、减少了8倍显存，通信容量与数据并行相同。内存减少与数据并行度和复杂度成线性关系。
<a id="__codelineno-22-28" name="__codelineno-22-28" href="#__codelineno-22-28"></a>
<a id="__codelineno-22-29" name="__codelineno-22-29" href="#__codelineno-22-29"></a>- ZeRO-2：ZeRO-2 是 DeepSpeed 中的一种优化器，可以实现模型并行、数据并行和模型的卸载。
<a id="__codelineno-22-30" name="__codelineno-22-30" href="#__codelineno-22-30"></a>
<a id="__codelineno-22-31" name="__codelineno-22-31" href="#__codelineno-22-31"></a>    ZeRO-2的原理：
<a id="__codelineno-22-32" name="__codelineno-22-32" href="#__codelineno-22-32"></a>      1、对优化器Optimizer、gradients进行分片
<a id="__codelineno-22-33" name="__codelineno-22-33" href="#__codelineno-22-33"></a>      2、Optimizer参数被分片，并安排在不同的rank上
<a id="__codelineno-22-34" name="__codelineno-22-34" href="#__codelineno-22-34"></a>      3、在backward过程中，gradients在不同的rank上独自进行reduce操作（取代了all-reduce，以此减少了通讯开销），每个rank独自更新各自负责的参数。
<a id="__codelineno-22-35" name="__codelineno-22-35" href="#__codelineno-22-35"></a>      4、在更新操作之后，广播或AllGather，保证所有的ranks接受到更新后的参数。
<a id="__codelineno-22-36" name="__codelineno-22-36" href="#__codelineno-22-36"></a>    ZeRO-2的优点：
<a id="__codelineno-22-37" name="__codelineno-22-37" href="#__codelineno-22-37"></a>      1、减少了8倍显存，通信容量与数据并行相同
<a id="__codelineno-22-38" name="__codelineno-22-38" href="#__codelineno-22-38"></a>
<a id="__codelineno-22-39" name="__codelineno-22-39" href="#__codelineno-22-39"></a>- ZeRO-1：ZeRO-1 是 DeepSpeed 中的一种优化器，可以实现模型并行、数据并行和模型的卸载。
<a id="__codelineno-22-40" name="__codelineno-22-40" href="#__codelineno-22-40"></a>
<a id="__codelineno-22-41" name="__codelineno-22-41" href="#__codelineno-22-41"></a>    ZeRO-1的原理：
<a id="__codelineno-22-42" name="__codelineno-22-42" href="#__codelineno-22-42"></a>      1、只对优化器Optimizer进行分片（与DDP过程相似）
<a id="__codelineno-22-43" name="__codelineno-22-43" href="#__codelineno-22-43"></a>      2、每个rank（gpu）单独负责 forward和backward过程，在完成backward后，梯度通过AllReduce来同步。
<a id="__codelineno-22-44" name="__codelineno-22-44" href="#__codelineno-22-44"></a>      3、每个rank只负责更新当前优化器分片的部分，由于每个rank只有部分分片的优化器state，所以当前rank会忽略其余的state。
<a id="__codelineno-22-45" name="__codelineno-22-45" href="#__codelineno-22-45"></a>      4、在更新优化器state后，通过广播或者AllGather的方式，确保所有的rank都收到最新更新过后的模型参数。
<a id="__codelineno-22-46" name="__codelineno-22-46" href="#__codelineno-22-46"></a>    ZeRO-1的优点：
<a id="__codelineno-22-47" name="__codelineno-22-47" href="#__codelineno-22-47"></a>      1、适合使用类似Adam进行优化的模型训练
<a id="__codelineno-22-48" name="__codelineno-22-48" href="#__codelineno-22-48"></a>      2、因为Adam拥有额外的参数m（momentum）与v（variance），特别是FP16混合精度训练。
<a id="__codelineno-22-49" name="__codelineno-22-49" href="#__codelineno-22-49"></a>      3、减少了4倍显存，通信容量与数据并行相同
<a id="__codelineno-22-50" name="__codelineno-22-50" href="#__codelineno-22-50"></a>    ZeRO-1的缺点：
<a id="__codelineno-22-51" name="__codelineno-22-51" href="#__codelineno-22-51"></a>      1、不适合使用SGD类似的优化器进行模型训练
<a id="__codelineno-22-52" name="__codelineno-22-52" href="#__codelineno-22-52"></a>      2、因为SGD只有较少的参数内存，并且由于需要更新模型参数，导致额外的通讯成本。
<a id="__codelineno-22-53" name="__codelineno-22-53" href="#__codelineno-22-53"></a>      3、只是解决了Optimizer state的冗余。
<a id="__codelineno-22-54" name="__codelineno-22-54" href="#__codelineno-22-54"></a>
<a id="__codelineno-22-55" name="__codelineno-22-55" href="#__codelineno-22-55"></a>- 其他优化器：DeepSpeed 还提供了其他优化器，例如 AdamOffload、Lamb、Larc、Lamb-Opt、Lamb-Opt-Offload、Lamb-Opt-ZeRO、Lamb-Opt-ZeRO-Offload、Lamb-Opt-ZeRO-3、Lamb-Opt-ZeRO-2、Lamb-Opt-ZeRO-1 等。
<a id="__codelineno-22-56" name="__codelineno-22-56" href="#__codelineno-22-56"></a>
<a id="__codelineno-22-57" name="__codelineno-22-57" href="#__codelineno-22-57"></a>与其他框架相比，DeepSpeed 的优点在于：
<a id="__codelineno-22-58" name="__codelineno-22-58" href="#__codelineno-22-58"></a>- 灵活性：DeepSpeed 使用动态图（Dynamic Graph）是指在运行时，根据输入数据的大小，自动调整计算图的结构，以节省内存。使得代码更加简洁易懂，更加灵活，适合于小规模数据和尝试实验。
<a id="__codelineno-22-59" name="__codelineno-22-59" href="#__codelineno-22-59"></a>- 速度：DeepSpeed 的计算速度更快，在相同的硬件上，DeepSpeed 的训练速度通常比TensorFlow、PyTorch快。
<a id="__codelineno-22-60" name="__codelineno-22-60" href="#__codelineno-22-60"></a>- 易用性：DeepSpeed 的接口和文档更加简单易懂，调试代码更加方便，并且有许多社区贡献的资源和工具。
<a id="__codelineno-22-61" name="__codelineno-22-61" href="#__codelineno-22-61"></a>- 自动并行：DeepSpeed 可以自动实现模型并行和数据并行，并提供分布式训练的支持。
<a id="__codelineno-22-62" name="__codelineno-22-62" href="#__codelineno-22-62"></a>- 自动混合精度训练：DeepSpeed 可以自动实现混合精度训练，并提供分布式训练的支持。
<a id="__codelineno-22-63" name="__codelineno-22-63" href="#__codelineno-22-63"></a>- 自动切分：DeepSpeed 可以自动实现模型切分，并提供分布式训练的支持。
<a id="__codelineno-22-64" name="__codelineno-22-64" href="#__codelineno-22-64"></a>- 自动混合精度优化器：DeepSpeed 可以自动实现混合精度优化器，并提供分布式训练的支持。
<a id="__codelineno-22-65" name="__codelineno-22-65" href="#__codelineno-22-65"></a>- 自动混合精度梯度累积：DeepSpeed 可以自动实现混合精度梯度累积，并提供分布式训练的支持。
<a id="__codelineno-22-66" name="__codelineno-22-66" href="#__codelineno-22-66"></a>- 弹性训练：DeepSpeed 可以实现弹性训练，可以自动扩展/缩减资源，以提高稳定性、吞吐量和资源利用率。
<a id="__codelineno-22-67" name="__codelineno-22-67" href="#__codelineno-22-67"></a>- 弹性扩缩容：DeepSpeed 可以实现弹性扩缩容，可以根据需要动态增加/减少节点，以应对突发的工作负载。
<a id="__codelineno-22-68" name="__codelineno-22-68" href="#__codelineno-22-68"></a>- 弹性调度：DeepSpeed 可以实现弹性调度，可以根据需要动态调度任务，以优化资源利用率和任务间的负载平衡。
<a id="__codelineno-22-69" name="__codelineno-22-69" href="#__codelineno-22-69"></a>
<a id="__codelineno-22-70" name="__codelineno-22-70" href="#__codelineno-22-70"></a>DeepSpeed的缺点：
<a id="__codelineno-22-71" name="__codelineno-22-71" href="#__codelineno-22-71"></a>- 易用性和灵活性带来的缺点是，DeepSpeed 在大型数据集上需要额外的工作来优化它的计算效率，而且同时也影响了代码的可维护性。
<a id="__codelineno-22-72" name="__codelineno-22-72" href="#__codelineno-22-72"></a>- DeepSpeed 缺乏安全性，因此有时可能会面临被恶意代码攻击的风险。
<a id="__codelineno-22-73" name="__codelineno-22-73" href="#__codelineno-22-73"></a>- 缺乏专业的深度学习库，导致其功能有限。
<a id="__codelineno-22-74" name="__codelineno-22-74" href="#__codelineno-22-74"></a>
<a id="__codelineno-22-75" name="__codelineno-22-75" href="#__codelineno-22-75"></a>DeepSpeed 可以通过以下方式加速模型的训练：
<a id="__codelineno-22-76" name="__codelineno-22-76" href="#__codelineno-22-76"></a>- 自动混合精度训练：通过混合精度训练，可以同时使用 FP16 和 FP32 两种数据类型，加速模型的训练，同时减少内存占用，提升模型的精度。
<a id="__codelineno-22-77" name="__codelineno-22-77" href="#__codelineno-22-77"></a>- 自动并行：通过模型并行和数据并行，可以将模型的不同部分分布到不同的 GPU 上，并行计算，提升模型的训练速度。
<a id="__codelineno-22-78" name="__codelineno-22-78" href="#__codelineno-22-78"></a>- 自动切分：通过切分模型，可以将模型的不同部分切分到不同的 GPU 上，并行计算，提升模型的训练速度。
<a id="__codelineno-22-79" name="__codelineno-22-79" href="#__codelineno-22-79"></a>- 自动混合精度优化器：通过混合精度优化器，可以同时使用 FP16 和 FP32 两种数据类型，加速模型的训练，同时减少内存占用，提升模型的精度。
<a id="__codelineno-22-80" name="__codelineno-22-80" href="#__codelineno-22-80"></a>- 自动混合精度梯度累积：通过混合精度梯度累积，可以同时使用 FP16 和 FP32 两种数据类型，加速模型的训练，同时减少内存占用，提升模型的精度。
<a id="__codelineno-22-81" name="__codelineno-22-81" href="#__codelineno-22-81"></a>
<a id="__codelineno-22-82" name="__codelineno-22-82" href="#__codelineno-22-82"></a>总的来说，DeepSpeed 是一个优秀的深度学习框架，适用于小规模数据和尝试实验，但在大型数据集上需要额外的工作来优化它的计算效率。
</code></pre></div>
<h4 id="dlrover">DLRover<a class="headerlink" href="#dlrover" title="Permanent link">&para;</a></h4>
<p>DLRover 是一个开源分布式工具，使大型 AI 模型的分布式训练变得简单、稳定、快速、绿色。它可以在分布式集群上自动训练深度学习模型，并提供自动扩展、缩容、弹性调度等功能，帮助开发人员专注于模型架构提升，而无需关心硬件加速、分布式运行等任何工程内容。可以帮助开发者在 K8s/Ray 上的深度学习训练作业中自动化运维。</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-23-1" name="__codelineno-23-1" href="#__codelineno-23-1"></a>DLRover的特点：
<a id="__codelineno-23-2" name="__codelineno-23-2" href="#__codelineno-23-2"></a>- 开源：DLRover 是一个开源项目，其代码可以免费获取。
<a id="__codelineno-23-3" name="__codelineno-23-3" href="#__codelineno-23-3"></a>- 简单易用：DLRover 使大型 AI 模型的分布式训练变得简单、稳定、快速、绿色。
<a id="__codelineno-23-4" name="__codelineno-23-4" href="#__codelineno-23-4"></a>- 容错性：DLRover 能够自动处理节点故障，分布式训练在发生故障时可以继续运行，并在几秒内从内存检查点恢复故障。
<a id="__codelineno-23-5" name="__codelineno-23-5" href="#__codelineno-23-5"></a>- Flash Checkpoint：DLRover 分布式训练可以在几秒内从内存检查点恢复故障。
<a id="__codelineno-23-6" name="__codelineno-23-6" href="#__codelineno-23-6"></a>- 自动扩展：DLRover 可以自动扩展/缩减资源，以提高稳定性、吞吐量和资源利用率。
<a id="__codelineno-23-7" name="__codelineno-23-7" href="#__codelineno-23-7"></a>- 弹性训练：DLRover 可以实现弹性训练，可以自动扩展/缩减资源，以提高稳定性、吞吐量和资源利用率。
<a id="__codelineno-23-8" name="__codelineno-23-8" href="#__codelineno-23-8"></a>- 弹性扩缩容：DLRover 可以实现弹性扩缩容，可以根据需要动态增加/减少节点，以应对突发的工作负载。
<a id="__codelineno-23-9" name="__codelineno-23-9" href="#__codelineno-23-9"></a>
<a id="__codelineno-23-10" name="__codelineno-23-10" href="#__codelineno-23-10"></a>DLRover 主要组件：
<a id="__codelineno-23-11" name="__codelineno-23-11" href="#__codelineno-23-11"></a>- 训练器（Trainer）：训练器（Trainer）是 DLRover 的核心组件，负责模型的训练。
<a id="__codelineno-23-12" name="__codelineno-23-12" href="#__codelineno-23-12"></a>- 任务管理器（Task Manager）：任务管理器（Task Manager）是 DLRover 的核心组件，负责任务的调度和管理。
<a id="__codelineno-23-13" name="__codelineno-23-13" href="#__codelineno-23-13"></a>- 资源管理器（Resource Manager）：资源管理器（Resource Manager）是 DLRover 的核心组件，负责资源的管理。
<a id="__codelineno-23-14" name="__codelineno-23-14" href="#__codelineno-23-14"></a>- 存储管理器（Storage Manager）：存储管理器（Storage Manager）是 DLRover 的核心组件，负责模型的存储和检索。
<a id="__codelineno-23-15" name="__codelineno-23-15" href="#__codelineno-23-15"></a>- 弹性调度器（Elastic Scheduler）：弹性调度器（Elastic Scheduler）是 DLRover 的核心组件，负责弹性调度。
<a id="__codelineno-23-16" name="__codelineno-23-16" href="#__codelineno-23-16"></a>- 弹性扩缩容器（Elastic Scaler）：弹性扩缩容器（Elastic Scaler）是 DLRover 的核心组件，负责弹性扩缩容。
<a id="__codelineno-23-17" name="__codelineno-23-17" href="#__codelineno-23-17"></a>- 弹性训练器（Elastic Trainer）：弹性训练器（Elastic Trainer）是 DLRover 的核心组件，负责弹性训练。
<a id="__codelineno-23-18" name="__codelineno-23-18" href="#__codelineno-23-18"></a>- 弹性存储器（Elastic Storage）：弹性存储器（Elastic Storage）是 DLRover 的核心组件，负责弹性存储。
<a id="__codelineno-23-19" name="__codelineno-23-19" href="#__codelineno-23-19"></a>- 弹性检查点（Elastic Checkpoint）：弹性检查点（Elastic Checkpoint）是 DLRover 的核心组件，负责弹性检查点。
<a id="__codelineno-23-20" name="__codelineno-23-20" href="#__codelineno-23-20"></a>- 弹性恢复器（Elastic Recovery）：弹性恢复器（Elastic Recovery）是 DLRover 的核心组件，负责弹性恢复。
<a id="__codelineno-23-21" name="__codelineno-23-21" href="#__codelineno-23-21"></a>- 弹性数据集（Elastic Dataset）：弹性数据集（Elastic Dataset）是 DLRover 的核心组件，负责弹性数据集。
<a id="__codelineno-23-22" name="__codelineno-23-22" href="#__codelineno-23-22"></a>- 弹性模型（Elastic Model）：弹性模型（Elastic Model）是 DLRover 的核心组件，负责弹性模型。
<a id="__codelineno-23-23" name="__codelineno-23-23" href="#__codelineno-23-23"></a>- 弹性优化器（Elastic Optimizer）：弹性优化器（Elastic Optimizer）是 DLRover 的核心组件，负责弹性优化器。
<a id="__codelineno-23-24" name="__codelineno-23-24" href="#__codelineno-23-24"></a>- 弹性混合精度（Elastic Mixed Precision）：弹性混合精度（Elastic Mixed Precision）是 DLRover 的核心组件，负责弹性混合精度。
<a id="__codelineno-23-25" name="__codelineno-23-25" href="#__codelineno-23-25"></a>- 弹性切分（Elastic Split）：弹性切分（Elastic Split）是 DLRover 的核心组件，负责弹性切分。
<a id="__codelineno-23-26" name="__codelineno-23-26" href="#__codelineno-23-26"></a>- 弹性混合精度优化器（Elastic Mixed Precision Optimizer）：弹性混合精度优化器（Elastic Mixed Precision Optimizer）是 DLRover 的核心组件，负责弹性混合精度优化器。
<a id="__codelineno-23-27" name="__codelineno-23-27" href="#__codelineno-23-27"></a>- 弹性混合精度梯度累积（Elastic Mixed Precision Gradient Accumulation）：弹性混合精度梯度累积（Elastic Mixed Precision Gradient Accumulation）是 DLRover 的核心组件，负责弹性混合精度梯度累积。
<a id="__codelineno-23-28" name="__codelineno-23-28" href="#__codelineno-23-28"></a>- 弹性混合精度训练（Elastic Mixed Precision Training）：弹性混合精度训练（Elastic Mixed Precision Training）是 DLRover 的核心组件，负责弹性混合精度训练。
<a id="__codelineno-23-29" name="__codelineno-23-29" href="#__codelineno-23-29"></a>- 弹性混合精度切分（Elastic Mixed Precision Split）：弹性混合精度切分（Elastic Mixed Precision Split）是 DLRover 的核心组件，负责弹性混合精度切分。
<a id="__codelineno-23-30" name="__codelineno-23-30" href="#__codelineno-23-30"></a>- 弹性混合精度优化器（Elastic Mixed Precision Optimizer）：弹性混合精度优化器（Elastic Mixed Precision Optimizer）是 DLRover 的核心组件，负责弹性混合精度优化器。
<a id="__codelineno-23-31" name="__codelineno-23-31" href="#__codelineno-23-31"></a>- 弹性混合精度梯度累积（Elastic Mixed Precision Gradient Accumulation）：弹性混合精度梯度累积（Elastic Mixed Precision Gradient Accumulation）是 DLRover 的核心组件，负责弹性混合精度梯度累积。
<a id="__codelineno-23-32" name="__codelineno-23-32" href="#__codelineno-23-32"></a>
<a id="__codelineno-23-33" name="__codelineno-23-33" href="#__codelineno-23-33"></a>DLRover 主要功能为：
<a id="__codelineno-23-34" name="__codelineno-23-34" href="#__codelineno-23-34"></a>- 自动化训练：DLRover 可以自动化地进行模型的训练，包括数据集的准备、模型的训练、模型的评估、模型的存储、模型的恢复等。
<a id="__codelineno-23-35" name="__codelineno-23-35" href="#__codelineno-23-35"></a>- 自动化运维：DLRover 可以自动化地进行模型的运维，包括模型的监控、模型的自动扩缩容、模型的自动调度等。
<a id="__codelineno-23-36" name="__codelineno-23-36" href="#__codelineno-23-36"></a>- 弹性训练：DLRover 可以实现弹性训练，可以自动扩展/缩减资源，以提高稳定性、吞吐量和资源利用率。
<a id="__codelineno-23-37" name="__codelineno-23-37" href="#__codelineno-23-37"></a>- 弹性扩缩容：DLRover 可以实现弹性扩缩容，可以根据需要动态增加/减少节点，以应对突发的工作负载。
<a id="__codelineno-23-38" name="__codelineno-23-38" href="#__codelineno-23-38"></a>- 弹性检查点：DLRover 可以实现弹性检查点，可以自动从故障中恢复训练状态。
<a id="__codelineno-23-39" name="__codelineno-23-39" href="#__codelineno-23-39"></a>- 弹性恢复：DLRover 可以实现弹性恢复，可以自动从故障中恢复训练状态。
</code></pre></div>
<h4 id="llama-factory">LLaMa-Factory<a class="headerlink" href="#llama-factory" title="Permanent link">&para;</a></h4>
<p>LLaMA-Factory 是一个易于使用的大规模语言模型（Large Language Model, LLM）微调框架，它支持多种模型，包括 LLaMA、BLOOM、Mistral、Baichuan、Qwen 和 ChatGLM 等。该框架旨在简化大型语言模型的微调过程，提供了一套完整的工具和接口，使得用户能够轻松地对预训练的模型进行定制化的训练和调整，以适应特定的应用场景。</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-24-1" name="__codelineno-24-1" href="#__codelineno-24-1"></a>LLaMA-Factory优点：
<a id="__codelineno-24-2" name="__codelineno-24-2" href="#__codelineno-24-2"></a>- 多种模型：LLaMA、Mistral、Mixtral-MoE、Qwen、Yi、Gemma、Baichuan、ChatGLM、Phi 等等。
<a id="__codelineno-24-3" name="__codelineno-24-3" href="#__codelineno-24-3"></a>- 集成方法：（增量）预训练、指令监督微调、奖励模型训练、PPO 训练、DPO 训练和 ORPO 训练。
<a id="__codelineno-24-4" name="__codelineno-24-4" href="#__codelineno-24-4"></a>- 多种精度：32 比特全参数微调、16 比特冻结微调、16 比特 LoRA 微调和基于 AQLM/AWQ/GPTQ/LLM.int8 的 2/4/8 比特 QLoRA 微调。
<a id="__codelineno-24-5" name="__codelineno-24-5" href="#__codelineno-24-5"></a>- 先进算法：GaLore、DoRA、LongLoRA、LLaMA Pro、LoRA+、LoftQ 和 Agent 微调。
<a id="__codelineno-24-6" name="__codelineno-24-6" href="#__codelineno-24-6"></a>- 实用技巧：FlashAttention-2、Unsloth、RoPE scaling、NEFTune 和 rsLoRA。
<a id="__codelineno-24-7" name="__codelineno-24-7" href="#__codelineno-24-7"></a>- 实验监控：LlamaBoard、TensorBoard、Wandb、MLflow 等等。
<a id="__codelineno-24-8" name="__codelineno-24-8" href="#__codelineno-24-8"></a>- 极速推理：基于 vLLM 的 OpenAI 风格 API、浏览器界面和命令行接口
<a id="__codelineno-24-9" name="__codelineno-24-9" href="#__codelineno-24-9"></a>- 数据集：LLaMA-Factory 目前只支持指定名称，位置放在项目的data目录下
<a id="__codelineno-24-10" name="__codelineno-24-10" href="#__codelineno-24-10"></a>- 模型压缩：LLaMA-Factory 提供了基于 AQLM/AWQ/GPTQ/LLM.int8 的 2/4/8 比特 QLoRA 微调，以实现模型的压缩和加速。
<a id="__codelineno-24-11" name="__codelineno-24-11" href="#__codelineno-24-11"></a>- 模型评估：LLaMA-Factory 提供了多种模型评估指标，如 PPL、BLEU、ROUGE、METEOR、SQuAD、GLUE、SST-2、MNLI、QQP、QNLI、RTE、MRPC、CoLA、STS-B、QQP、MNLI、RTE、MRPC、CoLA、STS-B 等。
<a id="__codelineno-24-12" name="__codelineno-24-12" href="#__codelineno-24-12"></a>
<a id="__codelineno-24-13" name="__codelineno-24-13" href="#__codelineno-24-13"></a>LLaMA-Factory缺点：
<a id="__codelineno-24-14" name="__codelineno-24-14" href="#__codelineno-24-14"></a>- 易用性：LLaMA-Factory 是一个易于使用的大规模语言模型微调框架，但仍然存在一些限制。
<a id="__codelineno-24-15" name="__codelineno-24-15" href="#__codelineno-24-15"></a>- 性能：LLaMA-Factory 目前的性能仍然不及原生框架，尤其是在大规模模型上。
<a id="__codelineno-24-16" name="__codelineno-24-16" href="#__codelineno-24-16"></a>- 稳定性：LLaMA-Factory 仍然存在一些限制，如内存泄漏、随机性、分布式训练等。
</code></pre></div>
<h3 id="_18">训练监控<a class="headerlink" href="#_18" title="Permanent link">&para;</a></h3>
<ul>
<li>训练监控（Training Monitoring）是指模型训练过程中，对模型的训练状态进行监控，以便及时发现和解决问题。</li>
<li>日志记录（Logging）是指记录模型训练过程中的信息，包括训练数据、模型参数、训练指标等。</li>
<li>训练可视化（Training Visualization）是指通过可视化的方式，展示模型训练过程中的信息，如训练数据、模型参数、训练指标等。</li>
<li>训练检查点（Training Checkpoint）是指保存模型训练过程中的信息，包括模型参数、训练指标等。</li>
<li>训练恢复（Training Recovery）是指在训练过程中，根据检查点恢复模型的训练状态。</li>
</ul>
<h4 id="swanlab">SwanLab<a class="headerlink" href="#swanlab" title="Permanent link">&para;</a></h4>
<p>SwanLab（SwanLab）平台（SwanLab Platform）是由华为推出的模型训练监控平台，可以实时监控模型的训练状态，并提供日志记录、训练可视化、训练检查点等功能。</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-25-1" name="__codelineno-25-1" href="#__codelineno-25-1"></a>import swan
<a id="__codelineno-25-2" name="__codelineno-25-2" href="#__codelineno-25-2"></a>swan.init(&quot;my-project&quot;, &quot;my-entity&quot;)
<a id="__codelineno-25-3" name="__codelineno-25-3" href="#__codelineno-25-3"></a>for epoch in range(10):
<a id="__codelineno-25-4" name="__codelineno-25-4" href="#__codelineno-25-4"></a>    # train
<a id="__codelineno-25-5" name="__codelineno-25-5" href="#__codelineno-25-5"></a>    #...
<a id="__codelineno-25-6" name="__codelineno-25-6" href="#__codelineno-25-6"></a>    # log metrics
<a id="__codelineno-25-7" name="__codelineno-25-7" href="#__codelineno-25-7"></a>    swan.log_metric(&quot;loss&quot;, loss)
<a id="__codelineno-25-8" name="__codelineno-25-8" href="#__codelineno-25-8"></a>    swan.log_metric(&quot;accuracy&quot;, accuracy)
</code></pre></div>
<h4 id="wandb">Wandb<a class="headerlink" href="#wandb" title="Permanent link">&para;</a></h4>
<p>Wandb（Weights &amp; Biases）平台（Wandb Platform）是由 Wandb 团队开发的模型训练监控平台，可以实时监控模型的训练状态，并提供日志记录、训练可视化、训练检查点等功能。</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-26-1" name="__codelineno-26-1" href="#__codelineno-26-1"></a>import wandb
<a id="__codelineno-26-2" name="__codelineno-26-2" href="#__codelineno-26-2"></a>wandb.init(project=&quot;my-project&quot;, entity=&quot;my-entity&quot;)
<a id="__codelineno-26-3" name="__codelineno-26-3" href="#__codelineno-26-3"></a>for epoch in range(10):
<a id="__codelineno-26-4" name="__codelineno-26-4" href="#__codelineno-26-4"></a>    # train
<a id="__codelineno-26-5" name="__codelineno-26-5" href="#__codelineno-26-5"></a>    #...
<a id="__codelineno-26-6" name="__codelineno-26-6" href="#__codelineno-26-6"></a>    # log metrics
<a id="__codelineno-26-7" name="__codelineno-26-7" href="#__codelineno-26-7"></a>    wandb.log({&quot;loss&quot;: loss, &quot;accuracy&quot;: accuracy})
</code></pre></div>
<h4 id="tensorboard">TensorBoard<a class="headerlink" href="#tensorboard" title="Permanent link">&para;</a></h4>
<p>TensorBoard 是 TensorFlow 官方推出的可视化工具，可以实时监控模型的训练状态，并提供日志记录、训练可视化、训练检查点等功能。</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-27-1" name="__codelineno-27-1" href="#__codelineno-27-1"></a>import torch.utils.tensorboard
<a id="__codelineno-27-2" name="__codelineno-27-2" href="#__codelineno-27-2"></a>from torch.utils.tensorboard import SummaryWriter
<a id="__codelineno-27-3" name="__codelineno-27-3" href="#__codelineno-27-3"></a>writer = SummaryWriter()
<a id="__codelineno-27-4" name="__codelineno-27-4" href="#__codelineno-27-4"></a>for epoch in range(10):
<a id="__codelineno-27-5" name="__codelineno-27-5" href="#__codelineno-27-5"></a>    # train
<a id="__codelineno-27-6" name="__codelineno-27-6" href="#__codelineno-27-6"></a>    #...
<a id="__codelineno-27-7" name="__codelineno-27-7" href="#__codelineno-27-7"></a>    # log metrics
<a id="__codelineno-27-8" name="__codelineno-27-8" href="#__codelineno-27-8"></a>    writer.add_scalar(&quot;loss&quot;, loss, epoch)
<a id="__codelineno-27-9" name="__codelineno-27-9" href="#__codelineno-27-9"></a>    writer.add_scalar(&quot;accuracy&quot;, accuracy, epoch)
</code></pre></div>
<h3 id="_19">评估模型<a class="headerlink" href="#_19" title="Permanent link">&para;</a></h3>
<p>模型评估是机器学习中的一个重要环节，它指的是对训练好的模型进行性能评估，以了解模型在未见过的新数据上的表现。这通常包括使用一系列指标来量化模型的预测能力、泛化能力、稳定性等。模型评估方法不针对模型本身，只针对问题和数据，因此可以用来评价来自不同方法的模型和泛化能力，进行用于部署的最终模型的选择。</p>
<ul>
<li>评估指标：评估指标（Evaluation Metric）是指模型训练时，用于评估模型性能的指标。
<div class="highlight"><pre><span></span><code><a id="__codelineno-28-1" name="__codelineno-28-1" href="#__codelineno-28-1"></a>作用：帮助选择最佳的超参数，避免模型在测试集上过拟合。
<a id="__codelineno-28-2" name="__codelineno-28-2" href="#__codelineno-28-2"></a>常用评估指标：
<a id="__codelineno-28-3" name="__codelineno-28-3" href="#__codelineno-28-3"></a>- 准确率（Accuracy）：准确率（Accuracy）是指模型预测正确的样本数与总样本数的比值。
<a id="__codelineno-28-4" name="__codelineno-28-4" href="#__codelineno-28-4"></a>- 精确率（Precision）：精确率（Precision）是指模型预测为正的样本中，真正为正的样本数与所有预测为正的样本数的比值。
<a id="__codelineno-28-5" name="__codelineno-28-5" href="#__codelineno-28-5"></a>- 召回率（Recall）：召回率（Recall）是指模型预测为正的样本中，真正为正的样本数与所有实际为正的样本数的比值。
<a id="__codelineno-28-6" name="__codelineno-28-6" href="#__codelineno-28-6"></a>- F1 值：F1 值（F1 Score）是指精确率和召回率的调和平均值。
<a id="__codelineno-28-7" name="__codelineno-28-7" href="#__codelineno-28-7"></a>- 损失函数值：损失函数值（Loss Value）是指模型训练过程中，损失函数计算得到的数值。
<a id="__codelineno-28-8" name="__codelineno-28-8" href="#__codelineno-28-8"></a>- 其他评估指标：其他评估指标（Other Evaluation Metric）是指模型训练时，用于评估模型性能的其他指标。
</code></pre></div></li>
<li>评估方法：评估方法（Evaluation Method）是指模型训练时，用于评估模型性能的方法。
<div class="highlight"><pre><span></span><code><a id="__codelineno-29-1" name="__codelineno-29-1" href="#__codelineno-29-1"></a>作用：帮助选择最佳的超参数，避免模型在测试集上过拟合。
<a id="__codelineno-29-2" name="__codelineno-29-2" href="#__codelineno-29-2"></a>常用评估方法：
<a id="__codelineno-29-3" name="__codelineno-29-3" href="#__codelineno-29-3"></a>- 交叉验证（Cross-Validation）：交叉验证（Cross-Validation）是指模型训练时，将数据集划分为训练集和验证集，使用验证集来评估模型的性能。
<a id="__codelineno-29-4" name="__codelineno-29-4" href="#__codelineno-29-4"></a>- 留出法（Hold-Out）：留出法（Hold-Out）是指模型训练时，将数据集划分为训练集和测试集，使用测试集来评估模型的性能。
<a id="__codelineno-29-5" name="__codelineno-29-5" href="#__codelineno-29-5"></a>- 其他评估方法：其他评估方法（Other Evaluation Method）是指模型训练时，用于评估模型性能的其他方法。
</code></pre></div></li>
<li>评估标准：评估标准（Evaluation Standard）是指模型训练时，用于评估模型性能的标准。
<div class="highlight"><pre><span></span><code><a id="__codelineno-30-1" name="__codelineno-30-1" href="#__codelineno-30-1"></a>作用：帮助选择最佳的超参数，避免模型在测试集上过拟合。
<a id="__codelineno-30-2" name="__codelineno-30-2" href="#__codelineno-30-2"></a>常用评估标准：
<a id="__codelineno-30-3" name="__codelineno-30-3" href="#__codelineno-30-3"></a>- 准确率（Accuracy）：准确率（Accuracy）是指模型预测正确的样本数与总样本数的比值。
<a id="__codelineno-30-4" name="__codelineno-30-4" href="#__codelineno-30-4"></a>- 精确率（Precision）：精确率（Precision）是指模型预测为正的样本中，真正为正的样本数与所有预测为正的样本数的比值。
<a id="__codelineno-30-5" name="__codelineno-30-5" href="#__codelineno-30-5"></a>- 召回率（Recall）：召回率（Recall）是指模型预测为正的样本中，真正为正的样本数与所有实际为正的样本数的比值。
<a id="__codelineno-30-6" name="__codelineno-30-6" href="#__codelineno-30-6"></a>- F1 值：F1 值（F1 Score）是指精确率和召回率的调和平均值。
<a id="__codelineno-30-7" name="__codelineno-30-7" href="#__codelineno-30-7"></a>- 其他评估标准：其他评估标准（Other Evaluation Standard）是指模型训练时，用于评估模型性能的其他标准。
</code></pre></div></li>
<li>评估模型：评估模型（Evaluating Model）是指模型训练时，用于评估模型性能的方法。
<div class="highlight"><pre><span></span><code><a id="__codelineno-31-1" name="__codelineno-31-1" href="#__codelineno-31-1"></a>作用：帮助选择最佳的超参数，避免模型在测试集上过拟合。
<a id="__codelineno-31-2" name="__codelineno-31-2" href="#__codelineno-31-2"></a>常用评估模型：
<a id="__codelineno-31-3" name="__codelineno-31-3" href="#__codelineno-31-3"></a>- 分类模型：分类模型（Classification Model）是指模型训练时，用于分类任务的模型。
<a id="__codelineno-31-4" name="__codelineno-31-4" href="#__codelineno-31-4"></a>- 回归模型：回归模型（Regression Model）是指模型训练时，用于回归任务的模型。
<a id="__codelineno-31-5" name="__codelineno-31-5" href="#__codelineno-31-5"></a>- 序列模型：序列模型（Sequence Model）是指模型训练时，用于序列任务的模型。
<a id="__codelineno-31-6" name="__codelineno-31-6" href="#__codelineno-31-6"></a>- 其他评估模型：其他评估模型（Other Evaluation Model）是指模型训练时，用于评估模型性能的其他模型。
</code></pre></div></li>
<li>评估结果：评估结果（Evaluation Result）是指模型训练时，用于评估模型性能的结果。
<div class="highlight"><pre><span></span><code><a id="__codelineno-32-1" name="__codelineno-32-1" href="#__codelineno-32-1"></a>作用：帮助选择最佳的超参数，避免模型在测试集上过拟合。
<a id="__codelineno-32-2" name="__codelineno-32-2" href="#__codelineno-32-2"></a>常用评估结果：
<a id="__codelineno-32-3" name="__codelineno-32-3" href="#__codelineno-32-3"></a>- 准确率（Accuracy）：准确率（Accuracy）是指模型预测正确的样本数与总样本数的比值。
<a id="__codelineno-32-4" name="__codelineno-32-4" href="#__codelineno-32-4"></a>- 精确率（Precision）：精确率（Precision）是指模型预测为正的样本中，真正为正的样本数与所有预测为正的样本数的比值。
<a id="__codelineno-32-5" name="__codelineno-32-5" href="#__codelineno-32-5"></a>- 召回率（Recall）：召回率（Recall）是指模型预测为正的样本中，真正为正的样本数与所有实际为正的样本数的比值。
<a id="__codelineno-32-6" name="__codelineno-32-6" href="#__codelineno-32-6"></a>- F1 值：F1 值（F1 Score）是指精确率和召回率的调和平均值。
<a id="__codelineno-32-7" name="__codelineno-32-7" href="#__codelineno-32-7"></a>- 损失函数值：损失函数值（Loss Value）是指模型训练过程中，损失函数计算得到的数值。
<a id="__codelineno-32-8" name="__codelineno-32-8" href="#__codelineno-32-8"></a>- 其他评估结果：其他评估结果（Other Evaluation Result）是指模型训练时，用于评估模型性能的其他结果。
</code></pre></div></li>
<li>评估过程：评估过程（Evaluation Process）是指模型训练时，用于评估模型性能的过程。
<div class="highlight"><pre><span></span><code><a id="__codelineno-33-1" name="__codelineno-33-1" href="#__codelineno-33-1"></a>作用：帮助选择最佳的超参数，避免模型在测试集上过拟合。
<a id="__codelineno-33-2" name="__codelineno-33-2" href="#__codelineno-33-2"></a>常用评估过程：
<a id="__codelineno-33-3" name="__codelineno-33-3" href="#__codelineno-33-3"></a>- 训练集验证（Training Set Validation）：训练集验证（Training Set Validation）是指模型训练时，使用训练集来评估模型的性能。
<a id="__codelineno-33-4" name="__codelineno-33-4" href="#__codelineno-33-4"></a>- 验证集验证（Validation Set Validation）：验证集验证（Validation Set Validation）是指模型训练时，使用验证集来评估模型的性能。
<a id="__codelineno-33-5" name="__codelineno-33-5" href="#__codelineno-33-5"></a>- 测试集验证（Test Set Validation）：测试集验证（Test Set Validation）是指模型训练时，使用测试集来评估模型的性能。
<a id="__codelineno-33-6" name="__codelineno-33-6" href="#__codelineno-33-6"></a>- 其他评估过程：其他评估过程（Other Evaluation Process）是指模型训练时，用于评估模型性能的其他过程。
</code></pre></div></li>
<li>评估工具：评估工具（Evaluation Tool）是指模型训练时，用于评估模型性能的工具。
<div class="highlight"><pre><span></span><code><a id="__codelineno-34-1" name="__codelineno-34-1" href="#__codelineno-34-1"></a>作用：帮助选择最佳的超参数，避免模型在测试集上过拟合。
<a id="__codelineno-34-2" name="__codelineno-34-2" href="#__codelineno-34-2"></a>常用评估工具：
<a id="__codelineno-34-3" name="__codelineno-34-3" href="#__codelineno-34-3"></a>- 准确率（Accuracy）：准确率（Accuracy）是指模型预测正确的样本数与总样本数的比值。
<a id="__codelineno-34-4" name="__codelineno-34-4" href="#__codelineno-34-4"></a>- 精确率（Precision）：精确率（Precision）是指模型预测为正的样本中，真正为正的样本数与所有预测为正的样本数的比值。
<a id="__codelineno-34-5" name="__codelineno-34-5" href="#__codelineno-34-5"></a>- 召回率（Recall）：召回率（Recall）是指模型预测为正的样本中，真正为正的样本数与所有实际为正的样本数的比值。
<a id="__codelineno-34-6" name="__codelineno-34-6" href="#__codelineno-34-6"></a>- F1 值：F1 值（F1 Score）是指精确率和召回率的调和平均值。
<a id="__codelineno-34-7" name="__codelineno-34-7" href="#__codelineno-34-7"></a>- 损失函数值：损失函数值（Loss Value）是指模型训练过程中，损失函数计算得到的数值。
<a id="__codelineno-34-8" name="__codelineno-34-8" href="#__codelineno-34-8"></a>- 其他评估工具：其他评估工具（Other Evaluation Tool）是指模型训练时，用于评估模型性能的其他工具。
</code></pre></div></li>
</ul>
<h3 id="_20">模型量化<a class="headerlink" href="#_20" title="Permanent link">&para;</a></h3>
<ul>
<li>模型量化（Model Quantization）是指对模型进行量化的方法，包括模型的量化、量化训练、量化推理等。</li>
<li>量化训练（Quantization Training）是指对模型进行量化训练的方法，包括量化训练的预处理、量化训练的训练、量化训练的评估等。</li>
<li>量化推理（Quantization Inference）是指对量化后的模型进行推理的方法，包括量化推理的预处理、量化推理的推理等。</li>
<li>量化（Quantization）是指对模型进行量化的方法，包括模型的量化、量化训练、量化推理等。</li>
</ul>
<h4 id="_21">量化训练<a class="headerlink" href="#_21" title="Permanent link">&para;</a></h4>
<ul>
<li>量化训练（Quantization Training）是指对模型进行量化训练的方法，包括量化训练的预处理、量化训练的训练、量化训练的评估等。</li>
<li>量化训练的预处理（Quantization Training Preprocessing）是指对量化训练的数据进行预处理的方法，包括数据清洗、数据转换、数据归一化等。</li>
<li>量化训练的训练（Quantization Training Training）是指对模型进行量化训练的方法，包括量化训练的训练、量化训练的评估等。</li>
<li>量化训练的评估（Quantization Training Evaluation）是指对量化训练的结果进行评估的方法，包括量化训练的指标、量化训练的性能评估等。</li>
</ul>
<h4 id="_22">量化推理<a class="headerlink" href="#_22" title="Permanent link">&para;</a></h4>
<ul>
<li>量化推理（Quantization Inference）是指对量化后的模型进行推理的方法，包括量化推理的预处理、量化推理的推理等。</li>
<li>量化推理的预处理（Quantization Inference Preprocessing）是指对量化推理的数据进行预处理的方法，包括数据清洗、数据转换、数据归一化等。</li>
<li>量化推理的推理（Quantization Inference Inference）是指对量化后的模型进行推理的方法。</li>
</ul>
<h2 id="_23">模型压缩<a class="headerlink" href="#_23" title="Permanent link">&para;</a></h2>
<p>模型压缩（Model Compression）是指通过减少模型的大小，包括模型的剪枝、模型的量化、模型的蒸馏、模型的压缩等，来提升模型的推理速度和降低模型的存储空间。</p>
<ul>
<li>量化（Quantization）是指将模型的权重量化，减少模型的大小，包括量化训练的预处理、量化训练的训练、量化训练的评估等，提升模型的推理速度和降低模型的存储空间。</li>
<li>剪枝（Pruning）是指通过裁剪模型的权重，减少模型的大小，包括模型的精度、模型的大小、模型的计算量等，提升模型的推理速度。</li>
<li>蒸馏（Distillation）是指通过蒸馏模型的知识，减少模型的大小，包括模型的精度、模型的大小、模型的计算量等，提升模型的推理速度。例如：将大模型的知识迁移到小模型，将大模型去蒸馏小模型。</li>
<li>量化-剪枝（Quantization-Pruning）是指将模型的权重量化，通过裁剪模型的权重，减少模型的大小，提升模型的推理速度。</li>
<li>量化-蒸馏（Quantization-Distillation）是指将模型的权重量化，通过蒸馏模型的知识，减少模型的大小，提升模型的推理速度。</li>
<li>剪枝-蒸馏（Pruning-Distillation）是指通过裁剪模型的权重，通过蒸馏模型的知识，减少模型的大小，提升模型的推理速度。</li>
<li>其他模型压缩方法：其他模型压缩方法（Other Model Compression Method）是指模型训练时，用于压缩模型的其他方法。</li>
</ul>
<h2 id="_24">加速推理<a class="headerlink" href="#_24" title="Permanent link">&para;</a></h2>
<p>加速推理（Accelerate Inference）是指在模型推理过程中，通过优化模型结构、优化模型参数、优化模型运行环境，提升模型的推理速度。</p>
<ul>
<li>DeepSpeed-MII 推理（DeepSpeed Model-Inference-Interface）是一种加速深度学习推理的技术，可以加速模型的推理。</li>
<li>VLLM 推理（Vector-Level Low-Memory）是一种加速深度学习推理的技术，可以加速模型的推理。</li>
<li>LightLLM 推理（Light-Level Low-Memory）是一种加速深度学习推理的技术，可以加速模型的推理。</li>
<li>TensorRT 推理（Tensor Runtine）是一种加速深度学习推理的技术，可以加速模型的推理。</li>
<li>ONNX 推理（Open Neural Network Exchange）是一种开源的模型格式，可以将 PyTorch 模型转换为其他框架，或用于模型推理。</li>
<li>TorchScript 推理（TorchScript）是一种将 PyTorch 模型转换为可执行代码的技术，可以加速模型的推理。</li>
<li>TensorFlow Lite 推理（TensorFlow Lite）是一种加速深度学习推理的技术，可以加速模型的推理。</li>
<li>模型量化（Model Quantization）是指将模型的权重量化，减少模型的大小，提升模型的推理速度。</li>
<li>模型剪枝（Model Pruning）是指通过裁剪模型的权重，减少模型的大小，提升模型的推理速度。</li>
<li>模型压缩（Model Compression）是指通过减少模型的大小，提升模型的推理速度。</li>
<li>DeepSpeed压缩器（DeepSpeed Compressor）是一种加速深度学习训练的技术，可以压缩模型的大小。</li>
</ul>
<h2 id="_25">参考资料<a class="headerlink" href="#_25" title="Permanent link">&para;</a></h2>
<ul>
<li><a href="https://mp.weixin.qq.com/s/y60j3">PyTorch 2.0 新特性：TensorBoard</a></li>
<li><a href="https://mp.weixin.qq.com/s/y60j3">PyTorch 2.0 新特性：动态图与静态图</a></li>
<li><a href="https://mp.weixin.qq.com/s/y60j3">PyTorch 2.0 新特性：模型量化</a></li>
<li><a href="https://mp.weixin.qq.com/s/y60j3">PyTorch 2.0 新特性：模型压缩</a></li>
<li><a href="https://mp.weixin.qq.com/s/y60j3">PyTorch 2.0 新特性：模型加速推理</a></li>
<li><a href="https://mp.weixin.qq.com/s/y60j3">PyTorch 2.0 新特性：模型加速训练</a></li>
</ul>
<h2 id="_26">后续更新<a class="headerlink" href="#_26" title="Permanent link">&para;</a></h2>
<p>本文档持续更新中，欢迎您持续关注，期待您的参与。</p>







  
    
  
  


  <aside class="md-source-file">
    
      
  <span class="md-source-file__fact">
    <span class="md-icon" title="最后更新">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M21 13.1c-.1 0-.3.1-.4.2l-1 1 2.1 2.1 1-1c.2-.2.2-.6 0-.8l-1.3-1.3c-.1-.1-.2-.2-.4-.2m-1.9 1.8-6.1 6V23h2.1l6.1-6.1-2.1-2M12.5 7v5.2l4 2.4-1 1L11 13V7h1.5M11 21.9c-5.1-.5-9-4.8-9-9.9C2 6.5 6.5 2 12 2c5.3 0 9.6 4.1 10 9.3-.3-.1-.6-.2-1-.2s-.7.1-1 .2C19.6 7.2 16.2 4 12 4c-4.4 0-8 3.6-8 8 0 4.1 3.1 7.5 7.1 7.9l-.1.2v1.8Z"/></svg>
    </span>
    <span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-iso_datetime">2024-07-13 17:03:07</span>
  </span>

    
    
    
    
  </aside>





                    <script src="../docs/assets/giscus_client.js"
                            data-repo="AISHU-Technology/kweaver-docs"
                            data-repo-id="R_kgDOH1U7qA"
                            data-category="General"
                            data-category-id="DIC_kwDOH1U7qM4CfonX"
                            data-mapping="pathname"
                            data-strict="1"
                            data-reactions-enabled="1"
                            data-emit-metadata="0"
                            data-input-position="bottom"
                            data-theme="preferred_color_scheme"
                            data-lang="zh-CN"
                            data-loading="lazy"
                            crossorigin="anonymous"
                            async>
                    </script>
                    
                </article>
            </div>
            
            
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
        <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12Z"/></svg>
  回到页面顶部
</button>
        
    </main>
    
    <footer class="md-footer">
  
    
      
      <nav class="md-footer__inner md-grid" aria-label="页脚" >
        
          
          <a href="../agents/" class="md-footer__link md-footer__link--prev" aria-label="上一页: Agent知识">
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
            </div>
            <div class="md-footer__title">
              <span class="md-footer__direction">
                上一页
              </span>
              <div class="md-ellipsis">
                Agent知识
              </div>
            </div>
          </a>
        
        
          
          <a href="../knowledge_graphs/" class="md-footer__link md-footer__link--next" aria-label="下一页: 知识图谱">
            <div class="md-footer__title">
              <span class="md-footer__direction">
                下一页
              </span>
              <div class="md-ellipsis">
                知识图谱
              </div>
            </div>
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11H4Z"/></svg>
            </div>
          </a>
        
      </nav>
    
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      Copyright &copy; 2023 - 2024 Martin Donath
    </div>
  
  
</div>
      
    </div>
  </div>
</footer>
    
</div>
<div class="md-dialog" data-md-component="dialog">
    <div class="md-dialog__inner md-typeset"></div>
</div>


<script id="__config" type="application/json">{"base": "../..", "features": ["navigation.indexes", "navigation.top", "search.highlight", "search.suggest", "search.share", "content.code.copy", "content.code.annotate", "navigation.footer", "navigation.tracking"], "search": "../../assets/javascripts/workers/search.f886a092.min.js", "translations": {"clipboard.copied": "\u5df2\u590d\u5236", "clipboard.copy": "\u590d\u5236", "search.result.more.one": "\u5728\u8be5\u9875\u4e0a\u8fd8\u6709 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.more.other": "\u5728\u8be5\u9875\u4e0a\u8fd8\u6709 # \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.none": "\u6ca1\u6709\u627e\u5230\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.one": "\u627e\u5230 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.other": "# \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.placeholder": "\u952e\u5165\u4ee5\u5f00\u59cb\u641c\u7d22", "search.result.term.missing": "\u7f3a\u5c11", "select.version": "\u9009\u62e9\u5f53\u524d\u7248\u672c"}, "version": {"provider": "mike"}}</script>


<script src="../../assets/javascripts/bundle.aecac24b.min.js"></script>

<script src="../../assets/extra.js"></script>

<script src="../../assets/mathjax.js"></script>

<script src="../../assets/tex-mml-chtml.js"></script>

<script src="../../assets/baidu-tongji.js"></script>

<script src="../../assets/busuanzi.pure.mini.js"></script>


</body>
</html>